{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "035__Advanced_Regular_Expressions.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMXQ6C23QvSDXq99JgOKC0X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rossel/DataQuest_Courses/blob/master/035__Advanced_Regular_Expressions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PWQ-EVGT2s_"
      },
      "source": [
        "# COURSE 5/6: DATA CLEANING IN PYTHON: ADVANCED"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THaX315BT2wD"
      },
      "source": [
        "# MISSION 2: Advanced Regular Expressions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHRw7Yfd630y"
      },
      "source": [
        "Describe complex patterns in text data for cleaning and analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQBrAPM9JEko"
      },
      "source": [
        "## 1. Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRUiFHPG7WWI"
      },
      "source": [
        "In the previous mission, we learned that regular expressions provide powerful ways to describe patterns in text that can help us clean and extract data. In this mission, we're going to build on those foundational principles, and learn:\n",
        "\n",
        "- Several new regex syntax components to allow us to express more complex criteria.\n",
        "- How to combine regular expression patterns to extract and transform data.\n",
        "- How to replace and clean data using regular expressions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-ORrStZ-NxL"
      },
      "source": [
        "We're going to continue working with the dataset from the previous mission from technology site [Hacker News](https://news.ycombinator.com/). Let's take a moment to refresh our memory of the different columns in this dataset:\n",
        "\n",
        "- `id`: The unique identifier from Hacker News for the story\n",
        "- `title`: The title of the story\n",
        "- `url`: The URL that the stories links to, if the story has a URL\n",
        "- `num_points`: The number of points the story acquired, calculated as the total number of upvotes minus the total number of downvotes\n",
        "- `num_comments`: The number of comments that were made on the story\n",
        "- `author`: The username of the person who submitted the story\n",
        "- `created_at`: The date and time at which the story was submitted\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbXiPkqn-gCV"
      },
      "source": [
        "We'll continue to analyze and count mentions of different programming languages in the dataset, and then we'll finish by extracting the different components of the URLs submitted to Hacker News.\n",
        "\n",
        "As we mentioned in the previous mission, you shouldn't expect to remember every single detail of regular expression syntax. The most important thing is to understand the core principles, what is possible, and where to look up the details. This will mean you can quickly jog your memory whenever you need regular expressions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ap1YlMRU-l5k"
      },
      "source": [
        "We'll be building on the foundational concepts that we learned in the previous mission. If you need to refresh any points of the syntax while you complete exercises in this mission, we recommend using a regex syntax reference like [RegExr](https://regexr.com/) so you can practice looking up syntax as you need it.\n",
        "\n",
        "Let's start by reading in the dataset using pandas and extracting the story titles from the `title` column:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zbhVrbR2n44"
      },
      "source": [
        "# Code to read csv file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQVsg_sh2o4j"
      },
      "source": [
        "# Once you have completed verification, go to the CSV file in Google Drive, right-click on it and select “Get shareable link”, and cut out the unique id in the link.\n",
        "# https://drive.google.com/file/d/1SgUoKVnxrer3-Yfvz4oBK0N9CzY6bJcu/view?usp=sharing\n",
        "id = \"1SgUoKVnxrer3-Yfvz4oBK0N9CzY6bJcu\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgFvCD-32wdh"
      },
      "source": [
        "# Download the dataset\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('hacker_news.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbX6uOxI1PGw"
      },
      "source": [
        "# import pandas library and read csv\n",
        "# extract the story titles from the title column\n",
        "import pandas as pd\n",
        "hn = pd.read_csv(\"hacker_news.csv\")\n",
        "titles = hn['title']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zvcwc6gABY3"
      },
      "source": [
        "In the story titles, we have two different capitalizations for the Python language: `Python` and `python`. In the previous mission, we learned two techniques for handling cases like these. The first is to use a set to match either `P` or `p`:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wx5sPmcZ-0U7",
        "outputId": "aa0c55a4-b639-417c-da88-16908f316702"
      },
      "source": [
        "pattern = r\"[Pp]ython\"\n",
        "python_counts = titles.str.contains(pattern).sum()\n",
        "print(python_counts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "160\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9-wjgNBAT7s"
      },
      "source": [
        "The second option we learned is to use `re.I` — the ignorecase flag — to make our pattern case insensitive:\n",
        "\n",
        "```\n",
        "pattern = r\"python\"\n",
        "python_counts = titles.str.contains(pattern, flags=re.I).sum()\n",
        "print(python_counts)\n",
        "```\n",
        "-> renders error: check"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K54G172cASMK"
      },
      "source": [
        "The ignorecase flag is particularly useful when we have many different capitalizations for a word or phrase. In our dataset, the SQL language has three different capitalizations: `SQL`, `sql`, and `Sql`.\n",
        "\n",
        "To use sets to capture all of these variations, we would need to use a set for each character:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "er24yAb8AZPA",
        "outputId": "770e4015-6a7f-4113-da27-b4c0cb031045"
      },
      "source": [
        "pattern = r\"[Ss][Qq][Ll]\"\n",
        "sql_counts = titles.str.contains(pattern).sum()\n",
        "print(sql_counts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "108\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbsq7BHhAp1t"
      },
      "source": [
        "Instead, let's use the ignorecase flag to write a case-insensitive version of this regular expression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbsywOC5Ares"
      },
      "source": [
        "**Instructions:**\n",
        "\n",
        "We have already imported pandas and re, read the CSV and extracted the title column.\n",
        "\n",
        "1. Create a case insensitive regex pattern that matches all case variations of the letters `SQL`.\n",
        "2. Use that regex pattern and the ignorecase flag to count the number of mentions of SQL in `titles`. Assign the result to `sql_counts`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6nETmYHAyzE"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Insert answer here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0qurPVJ7WZQ"
      },
      "source": [
        "## 2. Capture Groups"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TE82tWd37Wbk"
      },
      "source": [
        "In the previous exercise, we counted the number of mentions of \"SQL\" in the titles of stories. As we learned in the previous mission, to extract those mentions, we need to do two things:\n",
        "\n",
        "1. Use the `Series.str.extract()` [method](http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.extract.html).\n",
        "2. Use a regex capture group.\n",
        "\n",
        "We define a capture group by wrapping the part of our pattern we want to capture in parentheses. If we want to capture the whole pattern, we just wrap the whole pattern in a pair of parentheses:\n",
        "![img](https://s3.amazonaws.com/dq-content/369/single_capture_group.svg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urd6xT4gBA35"
      },
      "source": [
        "Let's look at how we can use a capture group to create a frequency table of the different capitalizations of SQL in our dataset. We start by wrapping our regex pattern in parentheses:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQwZtkq1BKb8"
      },
      "source": [
        "pattern = r\"(SQL)\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gy0ZdxtsBM2F"
      },
      "source": [
        "Next, we use `Series.str.extract()` to extract the different capitalizations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sayN3Ht2BVVM"
      },
      "source": [
        "sql_capitalizations = titles.str.extract(pattern, flags=re.I)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTi5Xv5iBack"
      },
      "source": [
        "Lastly, we use the `Series.value_counts()` [method](http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.value_counts.html) to create a frequency table of those capitalizations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XlD8MuzBgLy"
      },
      "source": [
        "sql_capitalizations_freq = sql_capitalizations.value_counts()\n",
        "print(sql_capitalizations_freq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Z8ckPDTBu-Q"
      },
      "source": [
        "We can extend this analysis by looking at titles that have letters immediately before the \"SQL,\" which is a convention often used to denote different variations or flavors of SQL:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4alkwOu2BxJE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d31cbe56-eb2b-40d5-b1e4-666ed48af5d9"
      },
      "source": [
        "pattern = r\"(\\w+SQL)\"\n",
        "sql_flavors = titles.str.extract(pattern, flags=re.I)\n",
        "sql_flavors_freq = sql_flavors.value_counts()\n",
        "print(sql_flavors_freq)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PostgreSQL    27\n",
            "NoSQL         16\n",
            "MySQL         12\n",
            "nosql          1\n",
            "mySql          1\n",
            "SparkSQL       1\n",
            "MemSQL         1\n",
            "CloudSQL       1\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPUmyUpHB17s"
      },
      "source": [
        "Notice how there is some duplication due to varied capitalization in this frequency table:\n",
        "\n",
        "- `NoSQL` and `nosql`\n",
        "- `MySQL` and `mysql`\n",
        "\n",
        "In this exercise, we're going to extract the mentions of different SQL flavors into a new column and clean those duplicates by making them all lowercase. We'll then analyze the results to look at the average number of comments for each flavor.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zyc3tm_9klI"
      },
      "source": [
        "**Instructions:**\n",
        "\n",
        "We have created a new dataframe, `hn_sql`, including only rows that mention a SQL flavor.\n",
        "\n",
        "1. Create a new column called `flavor` in the `hn_sql` dataframe, containing extracted mentions of SQL flavors, defined as:\n",
        " - Any time 'SQL' is preceded by one or more word characters.\n",
        " - Ignoring all case variation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VIGr20f-ACo"
      },
      "source": [
        "2. Use the `Series.str.lower()` [method](http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.lower.html#pandas.Series.str.lower) to clean the values in the `flavor` column by converting them to lowercase. Assign the values back to the column in `hn_sql`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqkQpkVu-P8f"
      },
      "source": [
        "3. Use the `DataFrame.pivot_table()` [method](http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pivot_table.html) to create a pivot table, `sql_pivot`.\n",
        " - The index of the pivot table should be the `flavor` column.\n",
        " - The values of the pivot table should be the mean of the `num_comments` column, aggregated by SQL flavor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EksqDM--CcHL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8XSkEAH7WlN"
      },
      "source": [
        "## 3. Using Capture Groups to Extract Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNo0Qpj77Wnu"
      },
      "source": [
        "So far we've used capture groups to extract all or most of the text in our regular expression pattern. Capture groups can also be useful to extract specific data from within our expression.\n",
        "\n",
        "Let's look at a sample of Hacker News titles that mention Python:\n",
        "\n",
        "```\n",
        "Developing a computational pipeline using the asyncio module in Python 3\n",
        "Python 3 on Google App Engine flexible environment now in beta\n",
        "Python 3.6 proposal, PEP 525: Asynchronous Generators\n",
        "How async/await works in Python 3.5.0\n",
        "Ubuntu Drops Python 2.7 from the Default Install in 16.04\n",
        "Show HN: First Release of Transcrypt Python3.5 to JavaScript Compiler\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSeHY5v_ag7q"
      },
      "source": [
        "All of these examples have a number after the word \"Python,\" which indicates a version number. Sometimes a space precedes the number, sometimes it doesn't. We can use the following regular expression to match these cases:\n",
        "\n",
        "![img](https://s3.amazonaws.com/dq-content/369/python_versions_fixed.svg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsryWOU4LoSn"
      },
      "source": [
        "We can use capture groups to extract the version of Python that is mentioned most often in our dataset by wrapping parentheses around the part of our regular expression which captures the version number.\n",
        "\n",
        "We'll use a capture group to capture the version number after the word \"Python,\" and then build a frequency table of the different versions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozqR5vwP1yr9"
      },
      "source": [
        "**Instructions:**\n",
        "\n",
        "1. Write a regular expression pattern which will match `Python` or `python`, followed by a space, followed by one or more digit characters or periods.\n",
        " - The regular expression should contain a capture group for the digit and period characters (the Python versions)\n",
        "2. Extract the Python versions from `titles` using the regular expression pattern.\n",
        "3. Use `Series.value_counts()` and the `dict()` function to create a dictionary frequency table of the extracted Python versions. Assign the result to `py_versions_freq`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BBi6rPO2xdk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8LUpO-E7Wqh"
      },
      "source": [
        "## 4. Counting Mentions of the 'C' Language"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f08FI_kK7WtG"
      },
      "source": [
        "So far, we've created regular expressions to clean and analyze the number of mentions of the Python, SQL, and Java languages. Next up: counting the mentions of the C language.\n",
        "\n",
        "We can start with a simple regular expression and then iterate as we find and exclude incorrect matches. Let's start with a simple regex that matches the letter \"c\" with word boundary anchors on either side:\n",
        "\n",
        "![img](https://s3.amazonaws.com/dq-content/369/c_regex_1.svg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Y7C0A1H3ExG"
      },
      "source": [
        "We'll re-use the `first_10_matches()` function that we defined in the previous mission to see the results we get from this regular expression:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rUPAL3c3J3n"
      },
      "source": [
        "def first_10_matches(pattern):\n",
        "    \"\"\"\n",
        "    Return the first 10 story titles that match\n",
        "    the provided regular expression\n",
        "    \"\"\"\n",
        "    all_matches = titles[titles.str.contains(pattern)]\n",
        "    first_10 = all_matches.head(10)\n",
        "    return first_10\n",
        "\n",
        "first_10_matches(r\"\\b[Cc]\\b\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gprEKDSg3SmB"
      },
      "source": [
        "Immediately, our results are reasonably relevant. However, we can quickly identify a few match types we want to prevent:\n",
        "\n",
        "- Mentions of C++, a distinct language from C.\n",
        "- Cases where the letter C is followed by a period, like in the substring `C.E.O.`\n",
        "\n",
        "Let's use a negative set to prevent matches for the `+` character and the `.` character.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rl0fakjr3afi"
      },
      "source": [
        "**Instructions:**\n",
        "\n",
        "We have provided a commented line of code containing the regular expression we used above.\n",
        "\n",
        "1. Uncomment the line of code. Add a negative set to the end of the regular expression that excludes:\n",
        "The period character `.`\n",
        "The plus character `+`.\n",
        "2. Use the `first_10_matches()` function to return the matches for the regular expression you built, assigning the result to `first_ten`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVN4f1403iCD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIZwf5Lc7W5R"
      },
      "source": [
        "## 5. Using Lookarounds to Control Matches Based on Surrounding Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEjo_u8w7W77"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YLcNyCp7W-W"
      },
      "source": [
        "## 6. BackReferences: Using Capture Groups in a RegEx Pattern"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puyaJoc27XBN"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PrE6tRh7XDq"
      },
      "source": [
        "## 7. Substituting Regular Expression Matches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0n68czZ7XGe"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4chfMrR7XJn"
      },
      "source": [
        "## 8. Extracting Domains from URLs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfzpLLwi9ge-"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOMu0TZ09gh6"
      },
      "source": [
        "## 9. Extracting URL Parts Using Multiple Capture Groups"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aqqhkben9gk_"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRVK3d_K9gpK"
      },
      "source": [
        "## 10. Using Named Capture Groups to Extract Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VE1YvPew9qjr"
      },
      "source": [
        ""
      ]
    }
  ]
}