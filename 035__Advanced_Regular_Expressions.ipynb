{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "035__Advanced_Regular_Expressions.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPMUkqv/CfSN7q0JqUsWOIR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rossel/DataQuest_Courses/blob/master/035__Advanced_Regular_Expressions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PWQ-EVGT2s_"
      },
      "source": [
        "# COURSE 5/6: DATA CLEANING IN PYTHON: ADVANCED"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THaX315BT2wD"
      },
      "source": [
        "# MISSION 2: Advanced Regular Expressions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHRw7Yfd630y"
      },
      "source": [
        "Describe complex patterns in text data for cleaning and analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQBrAPM9JEko"
      },
      "source": [
        "## 1. Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRUiFHPG7WWI"
      },
      "source": [
        "In the previous mission, we learned that regular expressions provide powerful ways to describe patterns in text that can help us clean and extract data. In this mission, we're going to build on those foundational principles, and learn:\n",
        "\n",
        "- Several new regex syntax components to allow us to express more complex criteria.\n",
        "- How to combine regular expression patterns to extract and transform data.\n",
        "- How to replace and clean data using regular expressions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-ORrStZ-NxL"
      },
      "source": [
        "We're going to continue working with the dataset from the previous mission from technology site [Hacker News](https://news.ycombinator.com/). Let's take a moment to refresh our memory of the different columns in this dataset:\n",
        "\n",
        "- `id`: The unique identifier from Hacker News for the story\n",
        "- `title`: The title of the story\n",
        "- `url`: The URL that the stories links to, if the story has a URL\n",
        "- `num_points`: The number of points the story acquired, calculated as the total number of upvotes minus the total number of downvotes\n",
        "- `num_comments`: The number of comments that were made on the story\n",
        "- `author`: The username of the person who submitted the story\n",
        "- `created_at`: The date and time at which the story was submitted\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbXiPkqn-gCV"
      },
      "source": [
        "We'll continue to analyze and count mentions of different programming languages in the dataset, and then we'll finish by extracting the different components of the URLs submitted to Hacker News.\n",
        "\n",
        "As we mentioned in the previous mission, you shouldn't expect to remember every single detail of regular expression syntax. The most important thing is to understand the core principles, what is possible, and where to look up the details. This will mean you can quickly jog your memory whenever you need regular expressions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ap1YlMRU-l5k"
      },
      "source": [
        "We'll be building on the foundational concepts that we learned in the previous mission. If you need to refresh any points of the syntax while you complete exercises in this mission, we recommend using a regex syntax reference like [RegExr](https://regexr.com/) so you can practice looking up syntax as you need it.\n",
        "\n",
        "Let's start by reading in the dataset using pandas and extracting the story titles from the `title` column:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zbhVrbR2n44"
      },
      "source": [
        "# Code to read csv file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQVsg_sh2o4j"
      },
      "source": [
        "# Once you have completed verification, go to the CSV file in Google Drive, right-click on it and select “Get shareable link”, and cut out the unique id in the link.\n",
        "# https://drive.google.com/file/d/1SgUoKVnxrer3-Yfvz4oBK0N9CzY6bJcu/view?usp=sharing\n",
        "id = \"1SgUoKVnxrer3-Yfvz4oBK0N9CzY6bJcu\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgFvCD-32wdh"
      },
      "source": [
        "# Download the dataset\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('hacker_news.csv')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbX6uOxI1PGw"
      },
      "source": [
        "# import pandas library and read csv\n",
        "# extract the story titles from the title column\n",
        "import pandas as pd\n",
        "hn = pd.read_csv(\"hacker_news.csv\")\n",
        "titles = hn['title']"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zvcwc6gABY3"
      },
      "source": [
        "In the story titles, we have two different capitalizations for the Python language: `Python` and `python`. In the previous mission, we learned two techniques for handling cases like these. The first is to use a set to match either `P` or `p`:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wx5sPmcZ-0U7",
        "outputId": "28158f8a-9c12-4772-d0e9-d38f8417dfb9"
      },
      "source": [
        "pattern = r\"[Pp]ython\"\n",
        "python_counts = titles.str.contains(pattern).sum()\n",
        "print(python_counts)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "160\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9-wjgNBAT7s"
      },
      "source": [
        "The second option we learned is to use `re.I` — the ignorecase flag — to make our pattern case insensitive:\n",
        "\n",
        "```\n",
        "pattern = r\"python\"\n",
        "python_counts = titles.str.contains(pattern, flags=re.I).sum()\n",
        "print(python_counts)\n",
        "```\n",
        "-> renders error: check"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K54G172cASMK"
      },
      "source": [
        "The ignorecase flag is particularly useful when we have many different capitalizations for a word or phrase. In our dataset, the SQL language has three different capitalizations: `SQL`, `sql`, and `Sql`.\n",
        "\n",
        "To use sets to capture all of these variations, we would need to use a set for each character:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "er24yAb8AZPA",
        "outputId": "14e78fa4-c9ef-4a3f-f8c6-6da3eac03e1a"
      },
      "source": [
        "pattern = r\"[Ss][Qq][Ll]\"\n",
        "sql_counts = titles.str.contains(pattern).sum()\n",
        "print(sql_counts)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "108\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbsq7BHhAp1t"
      },
      "source": [
        "Instead, let's use the ignorecase flag to write a case-insensitive version of this regular expression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbsywOC5Ares"
      },
      "source": [
        "**Instructions:**\n",
        "\n",
        "We have already imported pandas and re, read the CSV and extracted the title column.\n",
        "\n",
        "1. Create a case insensitive regex pattern that matches all case variations of the letters `SQL`.\n",
        "2. Use that regex pattern and the ignorecase flag to count the number of mentions of SQL in `titles`. Assign the result to `sql_counts`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6nETmYHAyzE"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Insert answer here"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0qurPVJ7WZQ"
      },
      "source": [
        "## 2. Capture Groups"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TE82tWd37Wbk"
      },
      "source": [
        "In the previous exercise, we counted the number of mentions of \"SQL\" in the titles of stories. As we learned in the previous mission, to extract those mentions, we need to do two things:\n",
        "\n",
        "1. Use the `Series.str.extract()` [method](http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.extract.html).\n",
        "2. Use a regex capture group.\n",
        "\n",
        "We define a capture group by wrapping the part of our pattern we want to capture in parentheses. If we want to capture the whole pattern, we just wrap the whole pattern in a pair of parentheses:\n",
        "![img](https://s3.amazonaws.com/dq-content/369/single_capture_group.svg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urd6xT4gBA35"
      },
      "source": [
        "Let's look at how we can use a capture group to create a frequency table of the different capitalizations of SQL in our dataset. We start by wrapping our regex pattern in parentheses:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQwZtkq1BKb8"
      },
      "source": [
        "pattern = r\"(SQL)\""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gy0ZdxtsBM2F"
      },
      "source": [
        "Next, we use `Series.str.extract()` to extract the different capitalizations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sayN3Ht2BVVM"
      },
      "source": [
        "sql_capitalizations = titles.str.extract(pattern, flags=re.I)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTi5Xv5iBack"
      },
      "source": [
        "Lastly, we use the `Series.value_counts()` [method](http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.value_counts.html) to create a frequency table of those capitalizations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XlD8MuzBgLy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c18c10f-9dca-4237-b82a-f01bad940293"
      },
      "source": [
        "sql_capitalizations_freq = sql_capitalizations.value_counts()\n",
        "print(sql_capitalizations_freq)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SQL    101\n",
            "Sql      4\n",
            "sql      3\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Z8ckPDTBu-Q"
      },
      "source": [
        "We can extend this analysis by looking at titles that have letters immediately before the \"SQL,\" which is a convention often used to denote different variations or flavors of SQL:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4alkwOu2BxJE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "041256f7-5a67-4b0b-d1b6-0cdb610e2628"
      },
      "source": [
        "pattern = r\"(\\w+SQL)\"\n",
        "sql_flavors = titles.str.extract(pattern, flags=re.I)\n",
        "sql_flavors_freq = sql_flavors.value_counts()\n",
        "print(sql_flavors_freq)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PostgreSQL    27\n",
            "NoSQL         16\n",
            "MySQL         12\n",
            "nosql          1\n",
            "mySql          1\n",
            "SparkSQL       1\n",
            "MemSQL         1\n",
            "CloudSQL       1\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPUmyUpHB17s"
      },
      "source": [
        "Notice how there is some duplication due to varied capitalization in this frequency table:\n",
        "\n",
        "- `NoSQL` and `nosql`\n",
        "- `MySQL` and `mysql`\n",
        "\n",
        "In this exercise, we're going to extract the mentions of different SQL flavors into a new column and clean those duplicates by making them all lowercase. We'll then analyze the results to look at the average number of comments for each flavor.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zyc3tm_9klI"
      },
      "source": [
        "**Instructions:**\n",
        "\n",
        "We have created a new dataframe, `hn_sql`, including only rows that mention a SQL flavor.\n",
        "\n",
        "1. Create a new column called `flavor` in the `hn_sql` dataframe, containing extracted mentions of SQL flavors, defined as:\n",
        " - Any time 'SQL' is preceded by one or more word characters.\n",
        " - Ignoring all case variation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VIGr20f-ACo"
      },
      "source": [
        "2. Use the `Series.str.lower()` [method](http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.lower.html#pandas.Series.str.lower) to clean the values in the `flavor` column by converting them to lowercase. Assign the values back to the column in `hn_sql`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqkQpkVu-P8f"
      },
      "source": [
        "3. Use the `DataFrame.pivot_table()` [method](http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pivot_table.html) to create a pivot table, `sql_pivot`.\n",
        " - The index of the pivot table should be the `flavor` column.\n",
        " - The values of the pivot table should be the mean of the `num_comments` column, aggregated by SQL flavor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EksqDM--CcHL"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8XSkEAH7WlN"
      },
      "source": [
        "## 3. Using Capture Groups to Extract Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNo0Qpj77Wnu"
      },
      "source": [
        "So far we've used capture groups to extract all or most of the text in our regular expression pattern. Capture groups can also be useful to extract specific data from within our expression.\n",
        "\n",
        "Let's look at a sample of Hacker News titles that mention Python:\n",
        "\n",
        "```\n",
        "Developing a computational pipeline using the asyncio module in Python 3\n",
        "Python 3 on Google App Engine flexible environment now in beta\n",
        "Python 3.6 proposal, PEP 525: Asynchronous Generators\n",
        "How async/await works in Python 3.5.0\n",
        "Ubuntu Drops Python 2.7 from the Default Install in 16.04\n",
        "Show HN: First Release of Transcrypt Python3.5 to JavaScript Compiler\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSeHY5v_ag7q"
      },
      "source": [
        "All of these examples have a number after the word \"Python,\" which indicates a version number. Sometimes a space precedes the number, sometimes it doesn't. We can use the following regular expression to match these cases:\n",
        "\n",
        "![img](https://s3.amazonaws.com/dq-content/369/python_versions_fixed.svg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsryWOU4LoSn"
      },
      "source": [
        "We can use capture groups to extract the version of Python that is mentioned most often in our dataset by wrapping parentheses around the part of our regular expression which captures the version number.\n",
        "\n",
        "We'll use a capture group to capture the version number after the word \"Python,\" and then build a frequency table of the different versions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozqR5vwP1yr9"
      },
      "source": [
        "**Instructions:**\n",
        "\n",
        "1. Write a regular expression pattern which will match `Python` or `python`, followed by a space, followed by one or more digit characters or periods.\n",
        " - The regular expression should contain a capture group for the digit and period characters (the Python versions)\n",
        "2. Extract the Python versions from `titles` using the regular expression pattern.\n",
        "3. Use `Series.value_counts()` and the `dict()` function to create a dictionary frequency table of the extracted Python versions. Assign the result to `py_versions_freq`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BBi6rPO2xdk"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8LUpO-E7Wqh"
      },
      "source": [
        "## 4. Counting Mentions of the 'C' Language"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f08FI_kK7WtG"
      },
      "source": [
        "So far, we've created regular expressions to clean and analyze the number of mentions of the Python, SQL, and Java languages. Next up: counting the mentions of the C language.\n",
        "\n",
        "We can start with a simple regular expression and then iterate as we find and exclude incorrect matches. Let's start with a simple regex that matches the letter \"c\" with word boundary anchors on either side:\n",
        "\n",
        "![img](https://s3.amazonaws.com/dq-content/369/c_regex_1.svg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Y7C0A1H3ExG"
      },
      "source": [
        "We'll re-use the `first_10_matches()` function that we defined in the previous mission to see the results we get from this regular expression:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rUPAL3c3J3n",
        "outputId": "bd3e576e-fe89-449c-cc29-b295f87c01d0"
      },
      "source": [
        "def first_10_matches(pattern):\n",
        "    \"\"\"\n",
        "    Return the first 10 story titles that match\n",
        "    the provided regular expression\n",
        "    \"\"\"\n",
        "    all_matches = titles[titles.str.contains(pattern)]\n",
        "    first_10 = all_matches.head(10)\n",
        "    return first_10\n",
        "\n",
        "first_10_matches(r\"\\b[Cc]\\b\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13                 Custom Deleters for C++ Smart Pointers\n",
              "220                        Lisp, C++: Sadness in my heart\n",
              "221                  MemSQL (YC W11) Raises $36M Series C\n",
              "353     VW C.E.O. Personally Apologized to President O...\n",
              "365                      The new C standards are worth it\n",
              "444           Moz raises $10m Series C from Foundry Group\n",
              "508     BDE 3.0 (Bloomberg's core C++ library): Open S...\n",
              "521          Fuchsia: Micro kernel written in C by Google\n",
              "549     How to Become a C.E.O.? The Quickest Path Is a...\n",
              "1282    A lightweight C++ signals and slots implementa...\n",
              "Name: title, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gprEKDSg3SmB"
      },
      "source": [
        "Immediately, our results are reasonably relevant. However, we can quickly identify a few match types we want to prevent:\n",
        "\n",
        "- Mentions of C++, a distinct language from C.\n",
        "- Cases where the letter C is followed by a period, like in the substring `C.E.O.`\n",
        "\n",
        "Let's use a negative set to prevent matches for the `+` character and the `.` character.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rl0fakjr3afi"
      },
      "source": [
        "**Instructions:**\n",
        "\n",
        "We have provided a commented line of code containing the regular expression we used above.\n",
        "\n",
        "1. Uncomment the line of code. Add a negative set to the end of the regular expression that excludes:\n",
        "The period character `.`\n",
        "The plus character `+`.\n",
        "2. Use the `first_10_matches()` function to return the matches for the regular expression you built, assigning the result to `first_ten`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVN4f1403iCD"
      },
      "source": [
        ""
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIZwf5Lc7W5R"
      },
      "source": [
        "## 5. Using Lookarounds to Control Matches Based on Surrounding Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEjo_u8w7W77"
      },
      "source": [
        "Let's look at the result of the previous exercise:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79hKIXLk35im"
      },
      "source": [
        "def first_10_matches(pattern):\n",
        "    \"\"\"\n",
        "    Return the first 10 story titles that match\n",
        "    the provided regular expression\n",
        "    \"\"\"\n",
        "    all_matches = titles[titles.str.contains(pattern)]\n",
        "    first_10 = all_matches.head(10)\n",
        "    return first_10\n",
        "\n",
        "# pattern = r\"\\b[Cc]\\b\"\n",
        "pattern = r\"\\b[Cc]\\b[^.+]\"\n",
        "first_ten = first_10_matches(pattern)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o82NnjZW4Lb0",
        "outputId": "d993e25b-2bc9-4e6b-819c-64469c94d5f7"
      },
      "source": [
        "print(first_ten)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "365                      The new C standards are worth it\n",
            "444           Moz raises $10m Series C from Foundry Group\n",
            "521          Fuchsia: Micro kernel written in C by Google\n",
            "1307            Show HN: Yupp, yet another C preprocessor\n",
            "1326                     The C standard formalized in Coq\n",
            "1365                          GNU C Library 2.23 released\n",
            "1429    Cysignals: signal handling (SIGINT, SIGSEGV, )...\n",
            "1620                        SDCC  Small Device C Compiler\n",
            "1949    Rewriting a Ruby C Extension in Rust: How a Na...\n",
            "2195    MyHTML  HTML Parser on Pure C with POSIX Threa...\n",
            "Name: title, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmlBEwo4iDTG"
      },
      "source": [
        "It looks like we're getting close. In our first 10 matches we have one irrelevant result, which is about \"Series C,\" a term used to represent a particular type of startup fundraising.\n",
        "\n",
        "Additionally, we've run into the same issue as we did in the previous mission — by using a negative set, we may have eliminated any instances where the last character of the title is \"C\" (the second last line of output matches in spite of the fact that it ends with \"C,\" because it also has \"C\" earlier in the string).\n",
        "\n",
        "Neither of these can be avoided using negative sets, which are used to allow multiple matches for a *single* character. Instead we'll need a new tool: **lookarounds**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlw9JwJPiY_Z"
      },
      "source": [
        "Lookarounds let us define a character or sequence of characters that either must or must not come before or after our regex match. There are four types of lookarounds:\n",
        "\n",
        "![img](https://s3.amazonaws.com/dq-content/369/lookarounds.svg)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6V3WcvDikPd"
      },
      "source": [
        "These tips can help you remember the syntax for lookarounds:\n",
        "\n",
        "- Inside the parentheses, the first character of a lookaround is always `?`.\n",
        "- If the lookaround is a **lookbehind**, the next character will be `<`, which you can think of as an arrow head pointing *behind* the match.\n",
        "- The next character indicates whether the lookaround is positive (`=`) or negative (`!`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXap7T-ni6hp"
      },
      "source": [
        "Let's create some test data that we'll use to illustrate how lookarounds work:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkW2ysTIi8sv"
      },
      "source": [
        "test_cases = ['Red_Green_Blue',\n",
        "              'Yellow_Green_Red',\n",
        "              'Red_Green_Red',\n",
        "              'Yellow_Green_Blue',\n",
        "              'Green']"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNpTll5Pi_Yi"
      },
      "source": [
        "We'll also create a function that will loop over our test cases and tell us whether our pattern matches. We'll use the re module rather than pandas since it tells us the exact text that matches, which will help us understand how the lookaround is working:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1iM9zpkjCcW"
      },
      "source": [
        "def run_test_cases(pattern):\n",
        "    for tc in test_cases:\n",
        "        result = re.search(pattern, tc)\n",
        "        print(result or \"NO MATCH\")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtk7eH3vjRSh"
      },
      "source": [
        "In each instance, we'll aim to match the substring `Green` depending on the characters that precede or follow it. Let's start by using a **positive lookahead **to include instances where the match is followed by the substring `_Blue`. We'll include the underscore character in the lookahead, otherwise we will get zero matches:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwEBP-ADjXMa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d402d6d-00c2-41f9-caba-59d013502133"
      },
      "source": [
        "run_test_cases(r\"Green(?=_Blue)\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<_sre.SRE_Match object; span=(4, 9), match='Green'>\n",
            "NO MATCH\n",
            "NO MATCH\n",
            "<_sre.SRE_Match object; span=(7, 12), match='Green'>\n",
            "NO MATCH\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHyRpaqnjdt8"
      },
      "source": [
        "Next we'll use a **positive lookbehind** to include instances where the match is preceded by the substring `Red_`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7VcScVajg-w",
        "outputId": "2061a98a-7893-4fbb-9bbc-b41e31611d74"
      },
      "source": [
        "run_test_cases(r\"(?<=Red_)Green\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<_sre.SRE_Match object; span=(4, 9), match='Green'>\n",
            "NO MATCH\n",
            "<_sre.SRE_Match object; span=(4, 9), match='Green'>\n",
            "NO MATCH\n",
            "NO MATCH\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGwEEjaYjq6I"
      },
      "source": [
        "And finally, using a **negative lookbehind** to include instances where the match isn't preceded by the substring `Yellow_`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_VdpnnAj0Ji",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d997ca09-ae27-4c0d-e5fe-be73a5e5eb89"
      },
      "source": [
        "run_test_cases(r\"(?<!Yellow_)Green\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<_sre.SRE_Match object; span=(4, 9), match='Green'>\n",
            "NO MATCH\n",
            "<_sre.SRE_Match object; span=(4, 9), match='Green'>\n",
            "NO MATCH\n",
            "<_sre.SRE_Match object; span=(0, 5), match='Green'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6S-X4i5At_Ze"
      },
      "source": [
        "The contents of a lookaround can include any other regular expression component. For instance, here is an example where we match only cases that are followed by exactly five characters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVCvYYQ7t-hT",
        "outputId": "8ae9a487-e889-4634-c60c-f31a91edcb83"
      },
      "source": [
        "run_test_cases(r\"Green(?=.{5})\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<_sre.SRE_Match object; span=(4, 9), match='Green'>\n",
            "NO MATCH\n",
            "NO MATCH\n",
            "<_sre.SRE_Match object; span=(7, 12), match='Green'>\n",
            "NO MATCH\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LD0--C41uDi0"
      },
      "source": [
        "The second and third test cases are followed by four characters, not five, and the last test case isn't followed by anything.\n",
        "\n",
        "Sometimes programming languages won't implement support for all lookarounds (notably, lookbehinds are not in the official JavaScript specification). As an example, to get full support in the [RegExr](https://regexr.com/) tool, you'll need to set it to use the PCRE regex engine.\n",
        "\n",
        "In this exercise, we're going to use lookarounds to refine the regular expression we build on the last screen to capture mentions of the \"C\" programming language. As a reminder, here is the last of the regular expressions we attempted to use with this exercise earlier, and the resultant titles that match:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fFxlgixuWbl",
        "outputId": "1f59041b-b87f-45b2-85b2-c73e8a5c2264"
      },
      "source": [
        "first_10_matches(r\"\\b[Cc]\\b[^.+]\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "365                      The new C standards are worth it\n",
              "444           Moz raises $10m Series C from Foundry Group\n",
              "521          Fuchsia: Micro kernel written in C by Google\n",
              "1307            Show HN: Yupp, yet another C preprocessor\n",
              "1326                     The C standard formalized in Coq\n",
              "1365                          GNU C Library 2.23 released\n",
              "1429    Cysignals: signal handling (SIGINT, SIGSEGV, )...\n",
              "1620                        SDCC  Small Device C Compiler\n",
              "1949    Rewriting a Ruby C Extension in Rust: How a Na...\n",
              "2195    MyHTML  HTML Parser on Pure C with POSIX Threa...\n",
              "Name: title, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8T6DDhmuiC1"
      },
      "source": [
        "Let's now use lookarounds to exclude the matches we don't want. We want to:\n",
        "\n",
        "- Keep excluding matches that are followed by `.` or `+`, but still match cases where \"C\" falls at the end of the sentence.\n",
        "- Exclude matches that have the word 'Series' immediately preceding them.\n",
        "\n",
        "This exercise is a little harder than those you've seen so far in this course — it's okay if it takes you a few attempts!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EtUKX14url9"
      },
      "source": [
        "**Instructions:**\n",
        "\n",
        "1. Write a regular expression and assign it to `pattern`. The regular expression should:\n",
        " - Match instances of `C` or `c` where they are not preceded or followed by another word character.\n",
        " - From the match above:\n",
        "    - Exclude instances where it is followed by a `.` or `+` character, without removing instances where the match occurs at the end of the **sentence**.\n",
        "    - Exclude instances where the word 'Series' immediately precedes the match.\n",
        "2. Count how many stories in `titles` match the regular expression. Assign the result to `c_mentions`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6llS6jYu_dI"
      },
      "source": [
        ""
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YLcNyCp7W-W"
      },
      "source": [
        "## 6. BackReferences: Using Capture Groups in a RegEx Pattern"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puyaJoc27XBN"
      },
      "source": [
        "Let's say we wanted to identify strings that had words with double letters, like the \"ee\" in \"feed.\" Because we don't know ahead of time what letters might be repeated, we need a way to specify a capture group and then to repeat it. We can do this with **backreferences**.\n",
        "\n",
        "Whenever we have one or more capture groups, we can refer to them using integers left to right as shown in this regex that matches the string `HelloGoodbye`:\n",
        "![img](https://s3.amazonaws.com/dq-content/369/backreference_syntax_1.svg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDGzfGTbvcER"
      },
      "source": [
        "Within a regular expression, we can use a backslash followed by that integer to refer to the group:\n",
        "![img](https://s3.amazonaws.com/dq-content/369/backreference_syntax_2.svg)\n",
        "\n",
        "The regular expression above will match the text `HelloGoodbyeGoodbyeHello`. Let's look at how we could write a regex to capture instances of the same two word characters in a row:\n",
        "\n",
        "![img](https://s3.amazonaws.com/dq-content/369/backreference_syntax_3.svg)\n",
        "\n",
        "Let's see this in action using Python:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHNS7G3Evxow",
        "outputId": "b9edbe85-9f31-439b-ee11-8fa457b45e66"
      },
      "source": [
        "test_cases = [\n",
        "              \"I'm going to read a book.\",\n",
        "              \"Green is my favorite color.\",\n",
        "              \"My name is Aaron.\",\n",
        "              \"No doubles here.\",\n",
        "              \"I have a pet eel.\"\n",
        "             ]\n",
        "\n",
        "for tc in test_cases:\n",
        "    print(re.search(r\"(\\w)\\1\", tc))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<_sre.SRE_Match object; span=(21, 23), match='oo'>\n",
            "<_sre.SRE_Match object; span=(2, 4), match='ee'>\n",
            "None\n",
            "None\n",
            "<_sre.SRE_Match object; span=(13, 15), match='ee'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsfBNemHv6kC"
      },
      "source": [
        "Notice that there was no match for the word `Aaron`, despite it containing a double \"a.\" This is because the uppercase and lowercase \"a\" are two different characters, so the backreference does not match.\n",
        "\n",
        "We can easily achieve the same thing using pandas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYKoQo8_v-nX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57d9bd6a-7111-4450-e9f5-98a806254603"
      },
      "source": [
        "test_cases = pd.Series(test_cases)\n",
        "print(test_cases.str.contains(r\"(\\w)\\1\"))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0     True\n",
            "1     True\n",
            "2    False\n",
            "3    False\n",
            "4     True\n",
            "dtype: bool\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/strings.py:2001: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
            "  return func(self, *args, **kwargs)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVqE0X0rwA9H"
      },
      "source": [
        "Let's use this technique to identify story titles that have repeated words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5bOGrAYwCPb"
      },
      "source": [
        "**Instructions:**\n",
        "\n",
        "1. Write a regular expression to match cases of repeated words:\n",
        " - We'll define a word as a series of one or more word *characters* preceded and followed by a boundary anchor.\n",
        " - We'll define repeated words as the same word repeated twice, *separated by a single whitespace character*.\n",
        "2. Select only the items in `titles` that match the regular expression. Assign the result to `repeated_words`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kfCpoBhwPDl"
      },
      "source": [
        ""
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PrE6tRh7XDq"
      },
      "source": [
        "## 7. Substituting Regular Expression Matches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0n68czZ7XGe"
      },
      "source": [
        "When we learned to work with basic string methods, we used the `str.replace()` [method](https://docs.python.org/3/library/stdtypes.html#str.replace) to replace simple substrings. We can achieve the same with regular expressions using the `re.sub()` [function](https://docs.python.org/3/library/re.html#re.sub). The basic syntax for `re.sub()` is:\n",
        "\n",
        "```\n",
        "re.sub(pattern, repl, string, flags=0)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrmsGWdsJf_-"
      },
      "source": [
        "The `repl` parameter is the text that you would like to substitute for the match. Let's look at a simple example where we replace all capital letters in a string with dashes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txodineSJjyg",
        "outputId": "042edb1e-9e00-4368-b33d-e713ccc6ef3b"
      },
      "source": [
        "string = \"aBcDEfGHIj\"\n",
        "\n",
        "print(re.sub(r\"[A-Z]\", \"-\", string))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a-c--f---j\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkZWNqLAR2cY"
      },
      "source": [
        "When working in pandas, we can use the `Series.str.replace()` [method](http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.replace.html), which uses nearly identical syntax:\n",
        "\n",
        "```\n",
        "Series.str.replace(pat, repl, flags=0)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9rny4vASOh5"
      },
      "source": [
        "Earlier, we discovered that there were multiple different capitalizations for SQL in our dataset. Let's look at how we could make these uniform with the `Series.str.replace()` method and a regular expression:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lb2OnBgQSVfX",
        "outputId": "ae73da61-94aa-4ba8-a7b8-430890c44be6"
      },
      "source": [
        "sql_variations = pd.Series([\"SQL\", \"Sql\", \"sql\"])\n",
        "\n",
        "sql_uniform = sql_variations.str.replace(r\"sql\", \"SQL\", flags=re.I)\n",
        "print(sql_uniform)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    SQL\n",
            "1    SQL\n",
            "2    SQL\n",
            "dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dL8Tk5XvSYJf"
      },
      "source": [
        "Let's use the same technique to make all the different variations of \"email\" in the dataset uniform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoM5Z_-bUIlF"
      },
      "source": [
        "**Instructions:**\n",
        "\n",
        "We have provided `email_variations`, a pandas Series containing all the variations of \"email\" in the dataset.\n",
        "\n",
        "1. Use a regular expression to replace each of the matches in `email_variations` with `\"email\"` and assign the result to `email_uniform`.\n",
        " - You may need to iterate several times when writing your regular expression in order to match every item.\n",
        "2. Use the same syntax to replace all mentions of email in `titles` with `\"email\"`. Assign the result to `titles_clean`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3t31LWZUUok"
      },
      "source": [
        ""
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOMu0TZ09gh6"
      },
      "source": [
        "## 8. Extracting Domains from URLs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aqqhkben9gk_"
      },
      "source": [
        "Over the final three screens in this mission, we'll extract components of URLs from our dataset. As a reminder, most stories on Hacker News contain a link to an external resource.\n",
        "\n",
        "The task we will be performing first is extracting the different components of the URLs in order to analyze them. On this screen, we'll start by extracting just the domains. Below is a list of some of the URLs in the dataset, with the domains highlighted in color, so you can see the part of the string we want to capture.\n",
        "\n",
        "![img](https://s3.amazonaws.com/dq-content/369/url_examples_1_updated.svg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7nwdZLPUcPS"
      },
      "source": [
        "The domain of each URL excludes the protocol (e.g. `https://`) and the page path (e.g. `/Technology-Ventures-Enterprise-Thomas-Byers/dp/0073523429`).\n",
        "\n",
        "There are several ways that you could use regular expressions to extract the domain, but we suggest the following technique:\n",
        "\n",
        "- Using a series of characters that will match the protocol.\n",
        "- Inside a capture group, using a set that will match the character classes used in the domain.\n",
        "- Because all of the URLs either end with the domain, or continue with page path which starts with `/` (a character not found in any domains), we don't need to cater for this part of the URL in our regular expression.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITcxt9gTUxqb"
      },
      "source": [
        "Once you have extracted the domains, you will be building a frequency table so we can determine the most popular domains. There are over 7,000 unique domains in our dataset, so to make the frequency table easier to analyze, we'll look at only the top 20 domains.\n",
        "\n",
        "We have provided some of the URLs from the dataset which will help you to iterate while you build your regular expression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xt5kESoyU3Gd"
      },
      "source": [
        "**Instructions:**\n",
        "\n",
        "1. Write a regular expression to extract the domains from `test_urls` and assign the result to `test_urls_clean`. We suggest the following technique:\n",
        " - Using a series of characters that will match the protocol.\n",
        " - Inside a capture group, using a set that will match the character classes used in the domain.\n",
        " - Because all of the URLs either end with the domain, or continue with page path which starts with `/` (a character not found in any domains), we don't need to cater for this part of the URL in our regular expression.\n",
        "2. Use the same regular expression to extract the domains from the `url` column of the `hn` dataframe. Assign the result to `domains`.\n",
        "3. Use `Series.value_counts()` to build a frequency table of the domains in `domains`, limiting the frequency table to just to the top 5. Assign the result to `top_domains`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWSHycB_VH_v"
      },
      "source": [
        ""
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EfPhoMAUXvg"
      },
      "source": [
        "## 9. Extracting URL Parts Using Multiple Capture Groups"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ppz7-u3UXvt"
      },
      "source": [
        "Having extracted just the domains from the URLs, on this final screen we'll extract each of the three component parts of the URLs:\n",
        "\n",
        "1. Protocol\n",
        "2. Domain\n",
        "3. Page path\n",
        "\n",
        "![img](https://s3.amazonaws.com/dq-content/369/url_examples_2_updated.svg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOop86gh5zs6"
      },
      "source": [
        "In order to do this, we'll create a regular expression with multiple capture groups. Multiple capture groups in regular expressions are defined the same way as single capture groups — using pairs of parentheses.\n",
        "\n",
        "Let's look at how this works using the first few values from the `created_at` column in our dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEzti7eM6L4e",
        "outputId": "c540b9e2-08b0-4a62-842b-3020e37da0a6"
      },
      "source": [
        "created_at = hn['created_at'].head()\n",
        "print(created_at)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0     8/4/2016 11:52\n",
            "1    6/23/2016 22:20\n",
            "2     6/17/2016 0:01\n",
            "3     9/30/2015 4:12\n",
            "4    10/31/2015 9:48\n",
            "Name: created_at, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frJOUJjq6xV3"
      },
      "source": [
        "We'll use capture groups to extract these dates and times into two columns:\n",
        "\n",
        "|||\n",
        "|---|---|\n",
        "|8/4/2016|\t11:52|\n",
        "|1/26/2016|\t19:30|\n",
        "|6/23/2016|\t22:20|\n",
        "6/17/2016|\t0:01|\n",
        "|9/30/2015|\t4:12|\n",
        "\n",
        "In order to do this we can write the following regular expression:\n",
        "\n",
        "![img](https://s3.amazonaws.com/dq-content/369/multiple_capture_groups.svg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47PEHzVi7bGu"
      },
      "source": [
        "Notice how we put a space character between the capture groups, which matches the space character in the original strings.\n",
        "\n",
        "Let's look at the result of using this regex pattern with `Series.str.extract()`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6VzOWVt7hBr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "977f7c20-a94e-496a-d364-d39a86c963e6"
      },
      "source": [
        "pattern = r\"(.+)\\s(.+)\"\n",
        "dates_times = created_at.str.extract(pattern)\n",
        "print(dates_times)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "            0      1\n",
            "0    8/4/2016  11:52\n",
            "1   6/23/2016  22:20\n",
            "2   6/17/2016   0:01\n",
            "3   9/30/2015   4:12\n",
            "4  10/31/2015   9:48\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afPtj-fg4GnA"
      },
      "source": [
        "The result is a dataframe with each of our capture groups defining a column of data.\n",
        "\n",
        "Now let's write a regular expression that will extract the URL components into individual columns of a dataframe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWMXQ_SI4IOs"
      },
      "source": [
        "**Instructions:**\n",
        "\n",
        "1. Write a regular expression that extracts URL components using three capture groups:\n",
        " - The first capture group should include the protocol text, up to but not including `://`.\n",
        " - The second group should contain the domain, from after `://` up to but not including `/`.\n",
        " - The third group should contain the page path, from after `/` to the end of the string.\n",
        "2. Use the regular expression pattern to extract the URL components from the `test_urls` series. Assign the results to `test_url_parts`.\n",
        "3. Use the regular expression pattern to extract the URL components from the `url` column of the `hn` dataframe. Assign the results to `url_parts`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ec2hsCn4c5t"
      },
      "source": [
        ""
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRVK3d_K9gpK"
      },
      "source": [
        "## 10. Using Named Capture Groups to Extract Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VE1YvPew9qjr"
      },
      "source": [
        "In the previous exercise, we created a regular expression which extracted the components from the story URLs into a dataframe with three columns:\n",
        "\n",
        "| |protocol|domain|path|\n",
        "|---|---|---|---|\n",
        "|0|http|www.interactivedynamicvideo.com| |\n",
        "|1|http|www.thewire.com|entertainment/2013/04/florida-djs-april-fools-water-joke/63798/ |\n",
        "|2|http|www.amazon.com |Technology-Ventures-Enterprise-Thomas-Byers/dp/0073523429 |\n",
        "|3|http| www.nytimes.com|www.nytimes.com\t2007/11/07/movies/07stein.html?_r=0|\n",
        "|4|http|arstechnica.com|arstechnica.com\tbusiness/2015/10/comcast-and-other-isps-boost-network-investment-despite-net-neutrality/ |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BwUJ6xP7-Jk"
      },
      "source": [
        "Our final task will be to name these columns, which we'll do using **named capture groups**. Let's look at the example from the previous screen where we used two capture groups to extract the date and time as two separate columns:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1GcLuvO8RTT",
        "outputId": "f41cdb1f-b4d3-4dcb-dadb-b6bab7e77a0e"
      },
      "source": [
        "created_at = hn['created_at'].head()\n",
        "\n",
        "pattern = r\"(.+) (.+)\"\n",
        "dates_times = created_at.str.extract(pattern)\n",
        "print(dates_times)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "            0      1\n",
            "0    8/4/2016  11:52\n",
            "1   6/23/2016  22:20\n",
            "2   6/17/2016   0:01\n",
            "3   9/30/2015   4:12\n",
            "4  10/31/2015   9:48\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aB6CVj_J8js9"
      },
      "source": [
        "In order to name a capture group we use the syntax `?P<name>`, where `name` is the name of our capture group. This syntax goes after the open parentheses, but before the regex syntax that defines the capture group:\n",
        "![img](https://s3.amazonaws.com/dq-content/369/named_capture_groups.svg)\n",
        "\n",
        "Let's look at the result of this syntax using pandas:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kc44HjcI8ttp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f83b03d2-eb92-4905-de0d-4ebcfd75b2d1"
      },
      "source": [
        "pattern = r\"(?P<date>.+) (?P<time>.+)\"\n",
        "dates_times = created_at.str.extract(pattern)\n",
        "print(dates_times)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         date   time\n",
            "0    8/4/2016  11:52\n",
            "1   6/23/2016  22:20\n",
            "2   6/17/2016   0:01\n",
            "3   9/30/2015   4:12\n",
            "4  10/31/2015   9:48\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekfVAZrnktK-"
      },
      "source": [
        "Each column has a name corresponding to the name of the capture group it represents.\n",
        "\n",
        "Let's finish this mission by adding names to our capture group from the previous screen to create a dataframe with named columns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WalWSBM3kwS_"
      },
      "source": [
        "**Instructions:**\n",
        "\n",
        "We have provided the regex pattern from the previous screen's solution.\n",
        "\n",
        "1. Uncomment the regular expression pattern. Add names to each capture group:\n",
        " - The first capture group should be called `protocol`.\n",
        " - The second capture group should be called `domain`.\n",
        " - The third capture group should be called `path`.\n",
        "2. Use the regular expression pattern to extract three named columns of url components from the `url` column of the `hn` dataframe. Assign the result to `url_parts`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLdd8amglyvG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}