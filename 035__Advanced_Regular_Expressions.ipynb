{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "035__Advanced_Regular_Expressions.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMcyS6jzY7J41iGefRlsTUH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rossel/DataQuest_Courses/blob/master/035__Advanced_Regular_Expressions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PWQ-EVGT2s_"
      },
      "source": [
        "# COURSE 5/6: DATA CLEANING IN PYTHON: ADVANCED"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THaX315BT2wD"
      },
      "source": [
        "# MISSION 2: Advanced Regular Expressions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHRw7Yfd630y"
      },
      "source": [
        "Describe complex patterns in text data for cleaning and analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQBrAPM9JEko"
      },
      "source": [
        "## 1. Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRUiFHPG7WWI"
      },
      "source": [
        "In the previous mission, we learned that regular expressions provide powerful ways to describe patterns in text that can help us clean and extract data. In this mission, we're going to build on those foundational principles, and learn:\n",
        "\n",
        "- Several new regex syntax components to allow us to express more complex criteria.\n",
        "- How to combine regular expression patterns to extract and transform data.\n",
        "- How to replace and clean data using regular expressions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-ORrStZ-NxL"
      },
      "source": [
        "We're going to continue working with the dataset from the previous mission from technology site [Hacker News](https://news.ycombinator.com/). Let's take a moment to refresh our memory of the different columns in this dataset:\n",
        "\n",
        "- `id`: The unique identifier from Hacker News for the story\n",
        "- `title`: The title of the story\n",
        "- `url`: The URL that the stories links to, if the story has a URL\n",
        "- `num_points`: The number of points the story acquired, calculated as the total number of upvotes minus the total number of downvotes\n",
        "- `num_comments`: The number of comments that were made on the story\n",
        "- `author`: The username of the person who submitted the story\n",
        "- `created_at`: The date and time at which the story was submitted\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbXiPkqn-gCV"
      },
      "source": [
        "We'll continue to analyze and count mentions of different programming languages in the dataset, and then we'll finish by extracting the different components of the URLs submitted to Hacker News.\n",
        "\n",
        "As we mentioned in the previous mission, you shouldn't expect to remember every single detail of regular expression syntax. The most important thing is to understand the core principles, what is possible, and where to look up the details. This will mean you can quickly jog your memory whenever you need regular expressions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ap1YlMRU-l5k"
      },
      "source": [
        "We'll be building on the foundational concepts that we learned in the previous mission. If you need to refresh any points of the syntax while you complete exercises in this mission, we recommend using a regex syntax reference like [RegExr](https://regexr.com/) so you can practice looking up syntax as you need it.\n",
        "\n",
        "Let's start by reading in the dataset using pandas and extracting the story titles from the `title` column:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zbhVrbR2n44"
      },
      "source": [
        "# Code to read csv file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQVsg_sh2o4j"
      },
      "source": [
        "# Once you have completed verification, go to the CSV file in Google Drive, right-click on it and select “Get shareable link”, and cut out the unique id in the link.\n",
        "# https://drive.google.com/file/d/1SgUoKVnxrer3-Yfvz4oBK0N9CzY6bJcu/view?usp=sharing\n",
        "id = \"1SgUoKVnxrer3-Yfvz4oBK0N9CzY6bJcu\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgFvCD-32wdh"
      },
      "source": [
        "# Download the dataset\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('hacker_news.csv')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbX6uOxI1PGw"
      },
      "source": [
        "# import pandas library and read csv\n",
        "# extract the story titles from the title column\n",
        "import pandas as pd\n",
        "hn = pd.read_csv(\"hacker_news.csv\")\n",
        "titles = hn['title']"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zvcwc6gABY3"
      },
      "source": [
        "In the story titles, we have two different capitalizations for the Python language: `Python` and `python`. In the previous mission, we learned two techniques for handling cases like these. The first is to use a set to match either `P` or `p`:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wx5sPmcZ-0U7",
        "outputId": "3a82069f-6185-4443-f534-2c7c5bd95e03"
      },
      "source": [
        "pattern = r\"[Pp]ython\"\n",
        "python_counts = titles.str.contains(pattern).sum()\n",
        "print(python_counts)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "160\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9-wjgNBAT7s"
      },
      "source": [
        "The second option we learned is to use `re.I` — the ignorecase flag — to make our pattern case insensitive:\n",
        "\n",
        "```\n",
        "pattern = r\"python\"\n",
        "python_counts = titles.str.contains(pattern, flags=re.I).sum()\n",
        "print(python_counts)\n",
        "```\n",
        "-> renders error: check"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K54G172cASMK"
      },
      "source": [
        "The ignorecase flag is particularly useful when we have many different capitalizations for a word or phrase. In our dataset, the SQL language has three different capitalizations: `SQL`, `sql`, and `Sql`.\n",
        "\n",
        "To use sets to capture all of these variations, we would need to use a set for each character:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "er24yAb8AZPA",
        "outputId": "d617e0f9-6ff8-4ab9-a11d-694dd00023b7"
      },
      "source": [
        "pattern = r\"[Ss][Qq][Ll]\"\n",
        "sql_counts = titles.str.contains(pattern).sum()\n",
        "print(sql_counts)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "108\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbsq7BHhAp1t"
      },
      "source": [
        "Instead, let's use the ignorecase flag to write a case-insensitive version of this regular expression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbsywOC5Ares"
      },
      "source": [
        "**Instructions:**\n",
        "\n",
        "We have already imported pandas and re, read the CSV and extracted the title column.\n",
        "\n",
        "1. Create a case insensitive regex pattern that matches all case variations of the letters `SQL`.\n",
        "2. Use that regex pattern and the ignorecase flag to count the number of mentions of SQL in `titles`. Assign the result to `sql_counts`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6nETmYHAyzE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0qurPVJ7WZQ"
      },
      "source": [
        "## 2. Capture Groups"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TE82tWd37Wbk"
      },
      "source": [
        "In the previous exercise, we counted the number of mentions of \"SQL\" in the titles of stories. As we learned in the previous mission, to extract those mentions, we need to do two things:\n",
        "\n",
        "1. Use the `Series.str.extract()` [method](http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.extract.html).\n",
        "2. Use a regex capture group.\n",
        "\n",
        "We define a capture group by wrapping the part of our pattern we want to capture in parentheses. If we want to capture the whole pattern, we just wrap the whole pattern in a pair of parentheses:\n",
        "![img](https://s3.amazonaws.com/dq-content/369/single_capture_group.svg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urd6xT4gBA35"
      },
      "source": [
        "Let's look at how we can use a capture group to create a frequency table of the different capitalizations of SQL in our dataset. We start by wrapping our regex pattern in parentheses:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQwZtkq1BKb8"
      },
      "source": [
        "pattern = r\"(SQL)\""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gy0ZdxtsBM2F"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8XSkEAH7WlN"
      },
      "source": [
        "## 3. Using Capture Groups to Extract Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNo0Qpj77Wnu"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8LUpO-E7Wqh"
      },
      "source": [
        "## 4. Counting Mentions of the 'C' Language"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f08FI_kK7WtG"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIZwf5Lc7W5R"
      },
      "source": [
        "## 5. Using Lookarounds to Control Matches Based on Surrounding Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEjo_u8w7W77"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YLcNyCp7W-W"
      },
      "source": [
        "## 6. BackReferences: Using Capture Groups in a RegEx Pattern"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puyaJoc27XBN"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PrE6tRh7XDq"
      },
      "source": [
        "## 7. Substituting Regular Expression Matches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0n68czZ7XGe"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4chfMrR7XJn"
      },
      "source": [
        "## 8. Extracting Domains from URLs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfzpLLwi9ge-"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOMu0TZ09gh6"
      },
      "source": [
        "## 9. Extracting URL Parts Using Multiple Capture Groups"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aqqhkben9gk_"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRVK3d_K9gpK"
      },
      "source": [
        "## 10. Using Named Capture Groups to Extract Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VE1YvPew9qjr"
      },
      "source": [
        ""
      ]
    }
  ]
}