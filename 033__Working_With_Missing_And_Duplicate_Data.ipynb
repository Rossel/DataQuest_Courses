{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "033__Working_With_Missing_And_Duplicate_Data.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPh547AjV6PQ0ZbteJQBhHK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rossel/DataQuest_Courses/blob/master/033__Working_With_Missing_And_Duplicate_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PWQ-EVGT2s_"
      },
      "source": [
        "# COURSE 4/6: DATA CLEANING AND ANALYSIS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THaX315BT2wD"
      },
      "source": [
        "# MISSION 5: Working With Missing And Duplicate Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqvHISHhhbIr"
      },
      "source": [
        "Learn how to work with missing and duplicate data in pandas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQBrAPM9JEko"
      },
      "source": [
        "## 1. Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M31GBhiU1a7I"
      },
      "source": [
        "As we near the end of our course, we'll cover a topic that's essential to any data cleaning workflow - handling missing and duplicate data.\n",
        "\n",
        "Missing or duplicate data may exist in a data set for a number of different reasons. Sometimes, missing or duplicate data is introduced as we perform cleaning and transformation tasks such as:\n",
        "\n",
        "- Combining data\n",
        "- Reindexing data\n",
        "- Reshaping data\n",
        "\n",
        "Other times, it exists in the original data set for reasons such as:\n",
        "\n",
        "- User input error\n",
        "- Data storage or conversion issues\n",
        "\n",
        "In the case of missing values, they may also exist in the original data set to purposely indicate that data is unavailable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fg1PXGcrRcTU"
      },
      "source": [
        "In the Pandas Fundamentals course, we learned that there are various ways to handle missing data:\n",
        "\n",
        "- Remove any rows that have missing values.\n",
        "- Remove any columns that have missing values.\n",
        "- Fill the missing values with some other value.\n",
        "- Leave the missing values as is.\n",
        "\n",
        "In this mission, we'll explore each of these options in detail and learn when to use them. We'll work with the 2015, 2016, and 2017 World Happiness Reports again - more specifically, we'll combine them and clean missing values as we start to define a more complete data cleaning workflow. You can find the data sets [here](https://www.kaggle.com/unsdsn/world-happiness#2015.csv), along with descriptions of each of the columns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcPUH3P-R1a0"
      },
      "source": [
        "In this mission, we'll work with modified versions of the data sets. Each data set has already been updated so that each contains the same countries. For example, if a country appeared in the original 2015 report, but not in the original 2016 report, a row like the one below was added to the 2016 data set:\n",
        "\n",
        "|...|\n",
        "|-|\n",
        "\n",
        "You'll notice that we revisit some of the concepts we learned in previous missions, such as combining data and vectorized string methods. This is to start giving you a sense of how all the data cleaning concepts we've learned fit together and better prepare you to work on the guided project at the end of this course!\n",
        "\n",
        "Let's start by gathering information about the dataframes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbEUdqZvSClb"
      },
      "source": [
        "**Instructions:**\n",
        "\n",
        "We've already read in the modified 2015, 2016, and 2017 World Happiness Reports to the variables `happiness2015`, `happiness2016`, and `happiness2017`, respectively. We also updated each dataframe so that each contain the same countries, as described above.\n",
        "\n",
        "- Use the `DataFrame.shape` attribute to confirm the number of rows and columns for `happiness2015`, `happiness2016`, and `happiness2017`.\n",
        " - Assign the result for `happiness2015` to `shape_2015`.\n",
        " - Assign the result for `happiness2016` to `shape_2016`.\n",
        " - Assign the result for `happiness2017` to `shape_2017`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jx4vKeuxNWig",
        "outputId": "7c66df06-177f-45b3-a9a3-226be37811e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        }
      },
      "source": [
        "# Import files directly using Google Colab\n",
        "# Download the files from the links below:\n",
        "# wh_2015.csv: https://drive.google.com/file/d/1hGi74f9j_HLbNGTkIcphEaU84OkgWVPK/view?usp=sharing\n",
        "# wh_2016.csv: https://drive.google.com/file/d/13OJS16M5C4qUdumDkm29bLTra8QX1DBq/view?usp=sharing\n",
        "# wh_2017.csv: https://drive.google.com/file/d/1pNnEWXuwDZt1A4pMOvNXkD5mCNoDrbWg/view?usp=sharing\n",
        "\n",
        "from google.colab import files\n",
        "upload = files.upload()\n",
        "upload = files.upload()\n",
        "upload = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a39a7b0e-0059-429e-94ae-cd626a9a2f78\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a39a7b0e-0059-429e-94ae-cd626a9a2f78\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6666ae13fbdc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mupload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mupload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mupload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m   result = _output.eval_js(\n\u001b[1;32m     63\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[0;32m---> 64\u001b[0;31m           input_id=input_id, output_id=output_id))\n\u001b[0m\u001b[1;32m     65\u001b[0m   \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: TypeError: Cannot read property '_uploadFiles' of undefined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RblOqgYLi5Uu"
      },
      "source": [
        "# Import pandas and numpy libraries\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-13RwLZHNwum"
      },
      "source": [
        " # Read the csv files\n",
        " happiness2015 = pd.read_csv(\"wh_2015.csv\")\n",
        " happiness2016 = pd.read_csv(\"wh_2016.csv\")\n",
        " happiness2017 = pd.read_csv(\"wh_2017.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y46pZghDH81R"
      },
      "source": [
        "happiness2015.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4YFJj93-0R7"
      },
      "source": [
        "# result of exercise comes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG5S_8uQyncm"
      },
      "source": [
        "## 2. Identifying Missing Values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwtGaWuoynmN"
      },
      "source": [
        "In the last exercise, we confirmed that each data set contains the same number of rows.\n",
        "\n",
        "Recall that the dataframes were updated so that each contains the same countries, even if the happiness score, happiness rank, etc. were missing. However, that also means that each likely contains missing values, like the one we reviewed in the previous screen:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPzOHA4W-_dc"
      },
      "source": [
        "|...|\n",
        "|---|\n",
        "\n",
        "In pandas, missing values are generally represented by the `NaN` value, as seen in the dataframe above, or the `None` value.\n",
        "\n",
        "However, it's good to note that pandas will not automatically identify values such as `n/a`, `-`, or `--` as `NaN` or `None`, but they may also indicate data is missing. See [here](https://stackoverflow.com/questions/40011531/in-pandas-when-using-read-csv-how-to-assign-a-nan-to-a-value-thats-not-the#answer-40011736) for more information on how to use the `pd.read_csv()` function to read those values in as `NaN`.\n",
        "\n",
        "Once we ensure that all missing values were read in correctly, we can use the `Series.isnull()` [method](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.isnull.html) to identify rows with missing values:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42AXW2Mm--9F"
      },
      "source": [
        "missing = happiness2015['Happiness Score'].isnull()\n",
        "happiness2015[missing]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzcJM9UusX6b"
      },
      "source": [
        "However, when working with bigger data sets, it's easier to get a summary of the missing values as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AeWbWmtsZz1"
      },
      "source": [
        "happiness2015.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e4ol3Gmsem1"
      },
      "source": [
        "The result is a series in which:\n",
        "\n",
        "- The index contains the names of the columns in `happiness2015`.\n",
        "- The corresponding value is the number of null values in each column.\n",
        "\n",
        "In `happiness2015`, all columns except for the `Country` and `Year` columns have six missing values.\n",
        "\n",
        "Let's confirm the number of missing values in `happiness2016` and `happiness2017` next.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnNnPWRFsnXU"
      },
      "source": [
        "**Instructions:**\n",
        "\n",
        "- Use the `DataFrame.isnull()` and `DataFrame.sum()` methods to confirm the number of missing values in `happiness2016`. Assign the result to `missing_2016`.\n",
        "- Use the `DataFrame.isnull()` and `DataFrame.sum()` methods to confirm the number of missing values in `happiness2017`. Assign the result to `missing_2017`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aTCwXK9s0cz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4253doT2yno6"
      },
      "source": [
        "## 3. Correcting Data Cleaning Errors that Result in Missing Values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caYbRE9synsd"
      },
      "source": [
        "In the previous exercise, you should've confirmed that `happiness2016` and `happiness2017` also contain missing values in all columns except for `Country` and `Year`. It's good to check for missing values before transforming data to make sure we don't unintentionally introduce missing values.\n",
        "\n",
        "If we do introduce missing values after transforming data, we'll have to determine if the data is really missing or if it's the result of some kind of error. As we progress through this mission, we'll use the following workflow to clean our missing values, starting with checking for errors:\n",
        "1. Check for errors in data cleaning/transformation.\n",
        "2. Use data from additional sources to fill missing values.\n",
        "3. Drop row/column.\n",
        "4. Fill missing values with reasonable estimates computed from the available data.\n",
        "\n",
        "Let's return to a task we completed in a previous mission - combining the 2015, 2016, and 2017 World Happiness Reports. Recall that we can use the `pd.concat()` [function](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html) to combine them:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWBas9JBt-d-"
      },
      "source": [
        "combined = pd.concat([happiness2015, happiness2016, happiness2017], ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qNKbiAruDox"
      },
      "source": [
        "Next, let's check for missing values in `combined`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2x3OxvGuGSv"
      },
      "source": [
        "combined.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNnxAONCuLWf"
      },
      "source": [
        "We can see above that our dataframe has many missing values and these missing values follow a pattern. Most columns fall into one of the following categories:\n",
        "\n",
        "- 177 missing values (about 1/3 of the total values)\n",
        "- 337 missing values (about 2/3 of the total values)\n",
        "\n",
        "You may have also noticed that some of the column names differ only by punctuation, which caused the dataframes to be combined incorrectly:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P93QtXi9uP24"
      },
      "source": [
        "Trust (Government Corruption)\n",
        "Trust..Government.Corruption."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOiNwyRyuT99"
      },
      "source": [
        "In the next exercise, we'll update the column names to make them uniform and combine the dataframes again. To clean the column names, we recommend using a technique we haven't covered yet, described in [this Stack Overflow answer](https://stackoverflow.com/questions/39741429/pandas-replace-a-character-in-all-column-names).\n",
        "\n",
        "As you start to work on more data cleaning tasks, you'll inevitably encounter scenarios you don't know specifically how to handle. Stack Overflow is a great place to reference to get answers for these questions, as other people have likely already asked the same question and solicited answers.\n",
        "\n",
        "As a reminder, below is a list of common string methods you can use to clean the columns:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvniRDN7W9ak"
      },
      "source": [
        "|Method|\tDescription|\n",
        "|---|---|\n",
        "|Series.str.split()\t|Splits each element in the Series.\n",
        "|Series.str.strip()\t|Strips whitespace from each string in the Series.\n",
        "|Series.str.lower()\t|Converts strings in the Series to lowercase.\n",
        "|Series.str.upper()\t|Converts strings in the Series to uppercase.\n",
        "|Series.str.get()\t|Retrieves the ith element of each element in the Series.\n",
        "|Series.str.replace()\t|Replaces a regex or string in the Series with another string.\n",
        "|Series.str.cat()\t|Concatenates strings in a Series.\n",
        "|Series.str.extract()\t|Extracts substrings from the Series matching a regex pattern.\n",
        "\n",
        "Let's clean the column names next."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IfNhhACXSfp"
      },
      "source": [
        "**Instructions:**\n",
        "\n",
        "We've already updated the column names for `happiness2017`.\n",
        "\n",
        "- Update the columns names for `happiness2015` and `happiness2016` to match the formatting of the column names in `happiness2017`. Use the following criteria to rename the columns:\n",
        " - All letters should be uppercase.\n",
        " - There should be only one space between words.\n",
        " - There should be no parentheses in column names\n",
        " - For example, the `Health (Life Expectancy)` columns should both be renamed to `HEALTH LIFE EXPECTANCY`.\n",
        "- Use the DataFrame.isnull() and DataFrame.sum() methods to check for missing values. Assign the result to a variable named missing.\n",
        "-Use the `pd.concat()` function to combine `happiness2015`, `happiness2016`, and `happiness2017`. Set the `ignore_index` argument equal to `True` to reset the index in the resulting dataframe. Assign the result to `combined`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RWvYBV9X3uh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mk8ZE6lynyq"
      },
      "source": [
        "## 4. Visualizing Missing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jU3qYaCkyn6E"
      },
      "source": [
        "In the last exercise, we corrected some of the missing values by fixing the column names. Note that we could have cleaned the column names without changing the capitalization. It's good practice, however, to make the capitalization uniform, because a stray uppercase or lowercase letter could've reintroduced missing values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oARepyw6vTtt"
      },
      "source": [
        "We also confirmed there are still values missing:\n",
        "\n",
        "```\n",
        "COUNTRY                          0\n",
        "DYSTOPIA RESIDUAL               22\n",
        "ECONOMY GDP PER CAPITA          22\n",
        "FAMILY                          22\n",
        "FREEDOM                         22\n",
        "GENEROSITY                      22\n",
        "HAPPINESS RANK                  22\n",
        "HAPPINESS SCORE                 22\n",
        "HEALTH LIFE EXPECTANCY          22\n",
        "LOWER CONFIDENCE INTERVAL      335\n",
        "REGION                         177\n",
        "STANDARD ERROR                 334\n",
        "TRUST GOVERNMENT CORRUPTION     22\n",
        "UPPER CONFIDENCE INTERVAL      335\n",
        "WHISKER HIGH                   337\n",
        "WHISKER LOW                    337\n",
        "YEAR                             0\n",
        "dtype: int64\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0X_K8vuFvw4R"
      },
      "source": [
        "We can learn more about where these missing values are located by visualizing them with a [heatmap](https://seaborn.pydata.org/generated/seaborn.heatmap.html), a graphical representation of our data in which values are represented as colors. We'll use the seaborn library to create the heatmap.\n",
        "\n",
        "Note below that we first reset the index to be the `YEAR` column so that we'll be able to see the corresponding year on the left side of the heatmap:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnU6-T-Hv6YK"
      },
      "source": [
        "import seaborn as sns\n",
        "combined_updated = combined.set_index('YEAR')\n",
        "sns.heatmap(combined_updated.isnull(), cbar=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXZluEr9hPcm"
      },
      "source": [
        "To understand this visualization, imagine we took `combined`, highlighted missing values in light gray and all other values in black, and then shrunk it so that was could easily view the entire dataframe at once.\n",
        "\n",
        "Since we concatenated `happiness2015`, `happiness2016`, and `happiness2017` by stacking them, note that the top third of the dataframe corresponds to the 2015 data, the second third corresponds to the 2016 data, and the bottom third corresponds to the 2017 data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keS_rv0YhW2k"
      },
      "source": [
        "We can make the following observations:\n",
        "\n",
        "- No values are missing in the `COUNTRY` column.\n",
        "- There are some rows in the 2015, 2016, and 2017 data with missing values in all columns EXCEPT the `COUNTRY` column.\n",
        "- Some columns only have data populated for one year.\n",
        "- It looks like the `REGION` data is missing for the year 2017.\n",
        "\n",
        "Let's check that the last statement is correct in the next exercise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OL1JISlKhkpU"
      },
      "source": [
        "**Instructions:**\n",
        "\n",
        "- Confirm that the `REGION` column is missing from the 2017 data. Recall that there are 164 rows for the year 2017.\n",
        " - Select just the rows in `combined` in which the `YEAR` column equals 2017. Then, select just the `REGION` column. Assign the result to `regions_2017`.\n",
        " - Use the `Series.isnull()` and `Series.sum()` to calculate the total number of missing values in `regions_2017`, the `REGION` column for 2017. Assign the result to `missing`.\n",
        "- Use the variable inspector to view the results of `missing`. Are all 164 region values missing for the year 2017?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKSiJ77nhkV1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qij5F-6myoCu"
      },
      "source": [
        "## 5. Using Data From Additional Sources to Fill in Missing Values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXXuX7BxyoNd"
      },
      "source": [
        "In the last exercise, we confirmed that the `REGION` column is missing from the 2017 data. Since we need the regions to analyze our data, let's turn our attention there next.\n",
        "\n",
        "Before we drop or replace any values, let's first see if there's a way we can use other available data to correct the values.\n",
        "\n",
        "1. Check for errors in data cleaning/transformation.\n",
        "2. Use data from additional sources to fill missing values.\n",
        "3. Drop row/column.\n",
        "4. Fill missing values with reasonable estimates computed from the available data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imwV5T08s8VD"
      },
      "source": [
        "Recall once more that each year contains the same countries. Since the regions are fixed values - the region a country was assigned to in 2015 or 2016 won't change - we should be able to assign the 2015 or 2016 region to the 2017 row.\n",
        "\n",
        "In order to do so, we'll use the following strategy:\n",
        "\n",
        "1. Create a dataframe containing all of the countries and corresponding regions from the `happiness2015`, `happiness2016`, and `happiness2017` dataframes.\n",
        "2. Use the `pd.merge()` [function](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html) to assign the `REGION` in the dataframe above to the corresponding country in `combined`.\n",
        "3. The result will have two region columns - the original column with missing values will be named `REGION_x`. The updated column without missing values will be named `REGION_y`. We'll drop `REGION_x` to eliminate confusion.\n",
        "\n",
        "Note that there are other ways to complete this task. We encourage you to explore them on your own.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9u8hZdBgtSUl"
      },
      "source": [
        "**Instructions:**\n",
        "\n",
        "We've already created a dataframe named `regions` containing all of the countries and corresponding regions from the `happiness2015`, `happiness2016`, and `happiness2017` dataframes.\n",
        "\n",
        "- Use the `pd.merge()` function to assign the `REGION` in the `regions` dataframe to the corresponding country in `combined`.\n",
        " - Set the `left` parameter equal to `combined`.\n",
        " - Set the `right` parameter equal to `regions`.\n",
        " - Set the `on` parameter equal to `'COUNTRY'`.\n",
        " - Set the `how` parameter equal to `'left'` to make sure we don't drop any rows from `combined`.\n",
        " - Assign the result back to `combined`.\n",
        "- Use the `DataFrame.drop()` method to drop the original region column with missing values, now named `REGION_x`.\n",
        " - Pass `'REGION_x'` into the `df.drop()` method.\n",
        " - Set the `axis` parameter equal to `1`.\n",
        " - Assign the result back to `combined`.\n",
        "- Use the `DataFrame.isnull()` and `DataFrame.sum()` methods to check for missing values. Assign the result to a variable named `missing`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmTRc_vEtSqx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2nRcgHHyocw"
      },
      "source": [
        "## 6. Identifying Duplicates Values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z99ez5g9yotY"
      },
      "source": [
        "In the previous screen, we used the 2015 and 2016 data to fill in the missing region values for the 2017 data. Note that we renamed the corrected region column to `REGION` separately to avoid confusion in the following exercises.\n",
        "\n",
        "Before we decide how to handle the rest of our missing values, let's first check our dataframe for duplicate rows.\n",
        "\n",
        "We'll use the `DataFrame.duplicated()` [method](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.duplicated.html) to check for duplicate values. If no parameters are specified, the method will check for any rows in which **all** columns have the same values.\n",
        "\n",
        "Since we should only have one country for each year, we can be a little more thorough by defining rows with ONLY the same country and year as duplicates. To accomplish this, let's pass a list of the `COUNTRY` and `YEAR` column names into the `df.duplicated()` method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MzkboE4FZvn"
      },
      "source": [
        "dups = combined.duplicated(['COUNTRY', 'YEAR'])\n",
        "combined[dups]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0GYtSPuFQC5"
      },
      "source": [
        "Since the dataframe is empty, we can tell that there are no rows with exactly the same country AND year.\n",
        "\n",
        "However, one thing to keep in mind is that the `df.duplicated()` method will only look for exact matches, so if the capitalization for country names isn't exactly the same, they won't be identified as duplicates. To be extra thorough, we can first standardize the capitalization for the `COUNTRY` column and then check for duplicates again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DMURayjF3eg"
      },
      "source": [
        "**Instructions:**\n",
        "\n",
        "- Standardize the capitalization so that all the values in the `COUNTRY` column in `combined` are uppercase.\n",
        " - As an example, ``'India'` should be changed to `'INDIA'`.\n",
        "- Use the `df.duplicated()` method to identify any rows that have the same value in the `COUNTRY` and `YEAR` columns. Assign your result to `dups`.\n",
        "- Use `dups` to index `combined`. Print the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1rsjZt8Fxy3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1HaOxAWyo6D"
      },
      "source": [
        "## 7. Correcting Duplicates Values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYv7PFrWypGq"
      },
      "source": [
        "In the previous screen, we standardized the capitalization of the values in the `COUNTRY` column and identified that we actually do have three duplicate rows!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CwHRoY5GV1S"
      },
      "source": [
        "combined['COUNTRY'] = combined['COUNTRY'].str.upper()\n",
        "dups = combined.duplicated(['COUNTRY', 'YEAR'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SY9YmuVGXev"
      },
      "source": [
        "Let's inspect all the rows for `SOMALILAND REGION` in `combined`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJrlRIzyGcIM"
      },
      "source": [
        "combined[combined['COUNTRY'] == 'SOMALILAND REGION']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LT1RZss1Gf8-"
      },
      "source": [
        "Now, we can see that there are two rows for 2015, 2016, and 2017 each."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nT49q4BTGnoM"
      },
      "source": [
        "Next, let's use the `df.drop_duplicates()` [method](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop_duplicates.html) to drop the duplicate rows. Like the `df.duplicated()` method, the `df.drop_duplicates()` method will define duplicates as rows in which **all** columns have the same values. We'll have to specify that rows with the same values in only the `COUNTRY` and `YEAR` columns should be dropped.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySaGNf_NG4iw"
      },
      "source": [
        "It's also important to note that by default, the `drop_duplicates()` method will only keep the first duplicate row. To keep the last duplicate row, set the `keep` parameter to `'last'`. Sometimes, this will mean sorting the dataframe before dropping the duplicate rows.\n",
        "\n",
        "In our case, since the second duplicate row above contains more missing values than the first row, we'll keep the first row."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7whKOt4HGMf"
      },
      "source": [
        "**Instructions:**\n",
        "\n",
        "- Use the `df.drop_duplicates()` method to drop rows with more than one country for each year. Assign the result back to `combined`.\n",
        " - Pass a list containing the `COUNTRY` and `YEAR` columns into the `drop_duplicates()` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCFmhTkfHN-P"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2Y-anKrypRD"
      },
      "source": [
        "## 8. Handle Missing Values by Dropping Columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bY9j5Zq3ypdu"
      },
      "source": [
        "In the last exercise, we used the df.drop()` method to drop columns we don't need for our analysis.\n",
        "\n",
        "However, as you start working with bigger datasets, it can sometimes be tedious to create a long list of column names to drop. Instead we can use the `DataFrame.dropna()` [method](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html) to complete the same task.\n",
        "\n",
        "By default, the `dropna()` method will drop rows with any missing values. To drop columns, we can set the `axis` parameter equal to `1`, just like with the `df.drop()` method:\n",
        "\n",
        "```\n",
        "df.dropna(axis=1)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XlT1UHrMukv"
      },
      "source": [
        "However, this would result in dropping columns with any missing values - we only want to drop certain columns. Instead, we can also use the `thresh` parameter to only drop columns if they contain below a certain number of non-null values.\n",
        "\n",
        "So far, we've used the `df.isnull()` method to confirm the number of missing values in each column. To confirm the number of values that are NOT missing, we can use the `DataFrame.notnull()` [method](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.notnull.html):\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyGEtE_AM63u"
      },
      "source": [
        "combined.notnull().sum().sort_values()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElmokmPpM9zm"
      },
      "source": [
        "Above, we can see that the columns we'd like to drop - `LOWER CONFIDENCE INTERVAL`, `STANDARD ERROR`, `UPPER CONFIDENCE INTERVAL`, `WHISKER HIGH`, and `WHISKER LOW` - only contain between 155 and 158 non null values. As a result, we'll set the `thresh` parameter equal to 159 in the `df.dropna()` method to drop them.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYRKB9JDNFwn"
      },
      "source": [
        "**Instructions:**\n",
        "\n",
        "- Use the `df.dropna()` method to drop all columns in `combined` with 159 or less non null values.\n",
        " - Set the `thresh` argument equal to `159` and the `axis` parameter equal to `1`.\n",
        "- Use the `df.isnull()` and `df.sum()` methods to calculate the number of missing values for each column. Assign the result to `missing`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cU5uvUE-NYuN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iTlUUG4yptr"
      },
      "source": [
        "## 9. Handle Missing Values by Dropping Columns Continued"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAC3Hx5bzbBh"
      },
      "source": [
        "In the last exercise, we used the `df.drop()` method to drop columns we don't need for our analysis.\n",
        "\n",
        "However, as you start working with bigger datasets, it can sometimes be tedious to create a long list of column names to drop. Instead we can use the `DataFrame.dropna()` [method](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html) to complete the same task.\n",
        "\n",
        "By default, the `dropna()` method will drop rows with any missing values. To drop columns, we can set the `axis` parameter equal to `1`, just like with the `df.drop()` method:\n",
        "\n",
        "```\n",
        "df.dropna(axis=1)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0wCaOt33t-I"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2avhv1FSzbgG"
      },
      "source": [
        "## 10. Analyzing Missing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6FKXvq4zcEh"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeGQ5-KmzcOQ"
      },
      "source": [
        "## 11. Handling Missing Values with Imputation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5t-0vZQLzcVZ"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7CDAGoXzcdH"
      },
      "source": [
        "## 12. Dropping Rows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7AVxNgnzlMy"
      },
      "source": [
        ""
      ]
    }
  ]
}