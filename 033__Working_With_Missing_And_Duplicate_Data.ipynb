{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "033__Working_With_Missing_And_Duplicate_Data.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPlhpE9bJmEr7kXI0eVnUdX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rossel/DataQuest_Courses/blob/master/033__Working_With_Missing_And_Duplicate_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PWQ-EVGT2s_"
      },
      "source": [
        "# COURSE 4/6: DATA CLEANING AND ANALYSIS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THaX315BT2wD"
      },
      "source": [
        "# MISSION 5: Working With Missing And Duplicate Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqvHISHhhbIr"
      },
      "source": [
        "Learn how to work with missing and duplicate data in pandas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQBrAPM9JEko"
      },
      "source": [
        "## 1. Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M31GBhiU1a7I"
      },
      "source": [
        "As we near the end of our course, we'll cover a topic that's essential to any data cleaning workflow - handling missing and duplicate data.\n",
        "\n",
        "Missing or duplicate data may exist in a data set for a number of different reasons. Sometimes, missing or duplicate data is introduced as we perform cleaning and transformation tasks such as:\n",
        "\n",
        "- Combining data\n",
        "- Reindexing data\n",
        "- Reshaping data\n",
        "\n",
        "Other times, it exists in the original data set for reasons such as:\n",
        "\n",
        "- User input error\n",
        "- Data storage or conversion issues\n",
        "\n",
        "In the case of missing values, they may also exist in the original data set to purposely indicate that data is unavailable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fg1PXGcrRcTU"
      },
      "source": [
        "In the Pandas Fundamentals course, we learned that there are various ways to handle missing data:\n",
        "\n",
        "- Remove any rows that have missing values.\n",
        "- Remove any columns that have missing values.\n",
        "- Fill the missing values with some other value.\n",
        "- Leave the missing values as is.\n",
        "\n",
        "In this mission, we'll explore each of these options in detail and learn when to use them. We'll work with the 2015, 2016, and 2017 World Happiness Reports again - more specifically, we'll combine them and clean missing values as we start to define a more complete data cleaning workflow. You can find the data sets [here](https://www.kaggle.com/unsdsn/world-happiness#2015.csv), along with descriptions of each of the columns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcPUH3P-R1a0"
      },
      "source": [
        "In this mission, we'll work with modified versions of the data sets. Each data set has already been updated so that each contains the same countries. For example, if a country appeared in the original 2015 report, but not in the original 2016 report, a row like the one below was added to the 2016 data set:\n",
        "\n",
        "|...|\n",
        "|-|\n",
        "\n",
        "You'll notice that we revisit some of the concepts we learned in previous missions, such as combining data and vectorized string methods. This is to start giving you a sense of how all the data cleaning concepts we've learned fit together and better prepare you to work on the guided project at the end of this course!\n",
        "\n",
        "Let's start by gathering information about the dataframes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbEUdqZvSClb"
      },
      "source": [
        "**Instructions:**\n",
        "\n",
        "We've already read in the modified 2015, 2016, and 2017 World Happiness Reports to the variables `happiness2015`, `happiness2016`, and `happiness2017`, respectively. We also updated each dataframe so that each contain the same countries, as described above.\n",
        "\n",
        "- Use the `DataFrame.shape` attribute to confirm the number of rows and columns for `happiness2015`, `happiness2016`, and `happiness2017`.\n",
        " - Assign the result for `happiness2015` to `shape_2015`.\n",
        " - Assign the result for `happiness2016` to `shape_2016`.\n",
        " - Assign the result for `happiness2017` to `shape_2017`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jx4vKeuxNWig",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "93e2c1ca-f5bd-4f23-b43e-74a8b48e0641"
      },
      "source": [
        "# Import files directly using Google Colab\n",
        "# Download the files from the links below:\n",
        "# wh_2015.csv: https://drive.google.com/file/d/1hGi74f9j_HLbNGTkIcphEaU84OkgWVPK/view?usp=sharing\n",
        "# wh_2016.csv: https://drive.google.com/file/d/13OJS16M5C4qUdumDkm29bLTra8QX1DBq/view?usp=sharing\n",
        "# wh_2017.csv: https://drive.google.com/file/d/1pNnEWXuwDZt1A4pMOvNXkD5mCNoDrbWg/view?usp=sharing\n",
        "\n",
        "from google.colab import files\n",
        "upload = files.upload()\n",
        "upload = files.upload()\n",
        "upload = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0f73fd55-d980-4c3c-b583-0828d1426afc\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0f73fd55-d980-4c3c-b583-0828d1426afc\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving wh_2015.csv to wh_2015 (1).csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5a40b497-2680-4b08-bd8a-265a43f3f0df\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5a40b497-2680-4b08-bd8a-265a43f3f0df\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving wh_2016.csv to wh_2016 (1).csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-139fc07a-5f08-4e56-b2a9-44ff2367b7eb\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-139fc07a-5f08-4e56-b2a9-44ff2367b7eb\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving wh_2017.csv to wh_2017 (1).csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RblOqgYLi5Uu"
      },
      "source": [
        "# Import pandas and numpy libraries\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-13RwLZHNwum"
      },
      "source": [
        " # Read the csv files\n",
        " happiness2015 = pd.read_csv(\"wh_2015.csv\")\n",
        " happiness2016 = pd.read_csv(\"wh_2016.csv\")\n",
        " happiness2017 = pd.read_csv(\"wh_2017.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y46pZghDH81R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "outputId": "41497c28-2baf-4b9a-d2b1-f316895c8187"
      },
      "source": [
        "happiness2015.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Country</th>\n",
              "      <th>Region</th>\n",
              "      <th>Happiness Rank</th>\n",
              "      <th>Happiness Score</th>\n",
              "      <th>Standard Error</th>\n",
              "      <th>Economy (GDP per Capita)</th>\n",
              "      <th>Family</th>\n",
              "      <th>Health (Life Expectancy)</th>\n",
              "      <th>Freedom</th>\n",
              "      <th>Trust (Government Corruption)</th>\n",
              "      <th>Generosity</th>\n",
              "      <th>Dystopia Residual</th>\n",
              "      <th>Year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Switzerland</td>\n",
              "      <td>Western Europe</td>\n",
              "      <td>1.0</td>\n",
              "      <td>7.587</td>\n",
              "      <td>0.03411</td>\n",
              "      <td>1.39651</td>\n",
              "      <td>1.34951</td>\n",
              "      <td>0.94143</td>\n",
              "      <td>0.66557</td>\n",
              "      <td>0.41978</td>\n",
              "      <td>0.29678</td>\n",
              "      <td>2.51738</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Iceland</td>\n",
              "      <td>Western Europe</td>\n",
              "      <td>2.0</td>\n",
              "      <td>7.561</td>\n",
              "      <td>0.04884</td>\n",
              "      <td>1.30232</td>\n",
              "      <td>1.40223</td>\n",
              "      <td>0.94784</td>\n",
              "      <td>0.62877</td>\n",
              "      <td>0.14145</td>\n",
              "      <td>0.43630</td>\n",
              "      <td>2.70201</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Denmark</td>\n",
              "      <td>Western Europe</td>\n",
              "      <td>3.0</td>\n",
              "      <td>7.527</td>\n",
              "      <td>0.03328</td>\n",
              "      <td>1.32548</td>\n",
              "      <td>1.36058</td>\n",
              "      <td>0.87464</td>\n",
              "      <td>0.64938</td>\n",
              "      <td>0.48357</td>\n",
              "      <td>0.34139</td>\n",
              "      <td>2.49204</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Norway</td>\n",
              "      <td>Western Europe</td>\n",
              "      <td>4.0</td>\n",
              "      <td>7.522</td>\n",
              "      <td>0.03880</td>\n",
              "      <td>1.45900</td>\n",
              "      <td>1.33095</td>\n",
              "      <td>0.88521</td>\n",
              "      <td>0.66973</td>\n",
              "      <td>0.36503</td>\n",
              "      <td>0.34699</td>\n",
              "      <td>2.46531</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Canada</td>\n",
              "      <td>North America</td>\n",
              "      <td>5.0</td>\n",
              "      <td>7.427</td>\n",
              "      <td>0.03553</td>\n",
              "      <td>1.32629</td>\n",
              "      <td>1.32261</td>\n",
              "      <td>0.90563</td>\n",
              "      <td>0.63297</td>\n",
              "      <td>0.32957</td>\n",
              "      <td>0.45811</td>\n",
              "      <td>2.45176</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Country          Region  ...  Dystopia Residual  Year\n",
              "0  Switzerland  Western Europe  ...            2.51738  2015\n",
              "1      Iceland  Western Europe  ...            2.70201  2015\n",
              "2      Denmark  Western Europe  ...            2.49204  2015\n",
              "3       Norway  Western Europe  ...            2.46531  2015\n",
              "4       Canada   North America  ...            2.45176  2015\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4YFJj93-0R7"
      },
      "source": [
        "# Solution\n",
        "\n",
        "shape_2015 = happiness2015.shape\n",
        "shape_2016 = happiness2016.shape\n",
        "shape_2017 = happiness2017.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG5S_8uQyncm"
      },
      "source": [
        "## 2. Identifying Missing Values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwtGaWuoynmN"
      },
      "source": [
        "In the last exercise, we confirmed that each data set contains the same number of rows.\n",
        "\n",
        "Recall that the dataframes were updated so that each contains the same countries, even if the happiness score, happiness rank, etc. were missing. However, that also means that each likely contains missing values, like the one we reviewed in the previous screen:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPzOHA4W-_dc"
      },
      "source": [
        "|...|\n",
        "|---|\n",
        "\n",
        "In pandas, missing values are generally represented by the `NaN` value, as seen in the dataframe above, or the `None` value.\n",
        "\n",
        "However, it's good to note that pandas will not automatically identify values such as `n/a`, `-`, or `--` as `NaN` or `None`, but they may also indicate data is missing. See [here](https://stackoverflow.com/questions/40011531/in-pandas-when-using-read-csv-how-to-assign-a-nan-to-a-value-thats-not-the#answer-40011736) for more information on how to use the `pd.read_csv()` function to read those values in as `NaN`.\n",
        "\n",
        "Once we ensure that all missing values were read in correctly, we can use the `Series.isnull()` [method](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.isnull.html) to identify rows with missing values:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42AXW2Mm--9F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "outputId": "454cfbb8-ca1f-4c4a-c6f2-95c2f0fc949b"
      },
      "source": [
        "missing = happiness2015['Happiness Score'].isnull()\n",
        "happiness2015[missing]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Country</th>\n",
              "      <th>Region</th>\n",
              "      <th>Happiness Rank</th>\n",
              "      <th>Happiness Score</th>\n",
              "      <th>Standard Error</th>\n",
              "      <th>Economy (GDP per Capita)</th>\n",
              "      <th>Family</th>\n",
              "      <th>Health (Life Expectancy)</th>\n",
              "      <th>Freedom</th>\n",
              "      <th>Trust (Government Corruption)</th>\n",
              "      <th>Generosity</th>\n",
              "      <th>Dystopia Residual</th>\n",
              "      <th>Year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>158</th>\n",
              "      <td>Belize</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159</th>\n",
              "      <td>Namibia</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>160</th>\n",
              "      <td>Puerto Rico</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161</th>\n",
              "      <td>Somalia</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162</th>\n",
              "      <td>Somaliland Region</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>South Sudan</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Country Region  ...  Dystopia Residual  Year\n",
              "158             Belize    NaN  ...                NaN  2015\n",
              "159            Namibia    NaN  ...                NaN  2015\n",
              "160        Puerto Rico    NaN  ...                NaN  2015\n",
              "161            Somalia    NaN  ...                NaN  2015\n",
              "162  Somaliland Region    NaN  ...                NaN  2015\n",
              "163        South Sudan    NaN  ...                NaN  2015\n",
              "\n",
              "[6 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzcJM9UusX6b"
      },
      "source": [
        "However, when working with bigger data sets, it's easier to get a summary of the missing values as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AeWbWmtsZz1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1ed2a5b-1ef1-4285-d76a-f646f8beab27"
      },
      "source": [
        "happiness2015.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Country                          0\n",
              "Region                           6\n",
              "Happiness Rank                   6\n",
              "Happiness Score                  6\n",
              "Standard Error                   6\n",
              "Economy (GDP per Capita)         6\n",
              "Family                           6\n",
              "Health (Life Expectancy)         6\n",
              "Freedom                          6\n",
              "Trust (Government Corruption)    6\n",
              "Generosity                       6\n",
              "Dystopia Residual                6\n",
              "Year                             0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e4ol3Gmsem1"
      },
      "source": [
        "The result is a series in which:\n",
        "\n",
        "- The index contains the names of the columns in `happiness2015`.\n",
        "- The corresponding value is the number of null values in each column.\n",
        "\n",
        "In `happiness2015`, all columns except for the `Country` and `Year` columns have six missing values.\n",
        "\n",
        "Let's confirm the number of missing values in `happiness2016` and `happiness2017` next.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnNnPWRFsnXU"
      },
      "source": [
        "**Instructions:**\n",
        "\n",
        "- Use the `DataFrame.isnull()` and `DataFrame.sum()` methods to confirm the number of missing values in `happiness2016`. Assign the result to `missing_2016`.\n",
        "- Use the `DataFrame.isnull()` and `DataFrame.sum()` methods to confirm the number of missing values in `happiness2017`. Assign the result to `missing_2017`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aTCwXK9s0cz"
      },
      "source": [
        "# Solution\n",
        "\n",
        "missing_2016 = happiness2016.isnull().sum()\n",
        "missing_2017 = happiness2017.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4253doT2yno6"
      },
      "source": [
        "## 3. Correcting Data Cleaning Errors that Result in Missing Values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caYbRE9synsd"
      },
      "source": [
        "In the previous exercise, you should've confirmed that `happiness2016` and `happiness2017` also contain missing values in all columns except for `Country` and `Year`. It's good to check for missing values before transforming data to make sure we don't unintentionally introduce missing values.\n",
        "\n",
        "If we do introduce missing values after transforming data, we'll have to determine if the data is really missing or if it's the result of some kind of error. As we progress through this mission, we'll use the following workflow to clean our missing values, starting with checking for errors:\n",
        "1. Check for errors in data cleaning/transformation.\n",
        "2. Use data from additional sources to fill missing values.\n",
        "3. Drop row/column.\n",
        "4. Fill missing values with reasonable estimates computed from the available data.\n",
        "\n",
        "Let's return to a task we completed in a previous mission - combining the 2015, 2016, and 2017 World Happiness Reports. Recall that we can use the `pd.concat()` [function](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html) to combine them:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWBas9JBt-d-"
      },
      "source": [
        "combined = pd.concat([happiness2015, happiness2016, happiness2017], ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qNKbiAruDox"
      },
      "source": [
        "Next, let's check for missing values in `combined`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2x3OxvGuGSv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfd5af3f-b644-4cee-c372-fec1b1c22c36"
      },
      "source": [
        "combined.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Country                            0\n",
              "Region                           177\n",
              "Happiness Rank                   177\n",
              "Happiness Score                  177\n",
              "Standard Error                   334\n",
              "Economy (GDP per Capita)         177\n",
              "Family                            22\n",
              "Health (Life Expectancy)         177\n",
              "Freedom                           22\n",
              "Trust (Government Corruption)    177\n",
              "Generosity                        22\n",
              "Dystopia Residual                177\n",
              "Year                               0\n",
              "Lower Confidence Interval        335\n",
              "Upper Confidence Interval        335\n",
              "Happiness.Rank                   337\n",
              "Happiness.Score                  337\n",
              "Whisker.high                     337\n",
              "Whisker.low                      337\n",
              "Economy..GDP.per.Capita.         337\n",
              "Health..Life.Expectancy.         337\n",
              "Trust..Government.Corruption.    337\n",
              "Dystopia.Residual                337\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNnxAONCuLWf"
      },
      "source": [
        "We can see above that our dataframe has many missing values and these missing values follow a pattern. Most columns fall into one of the following categories:\n",
        "\n",
        "- 177 missing values (about 1/3 of the total values)\n",
        "- 337 missing values (about 2/3 of the total values)\n",
        "\n",
        "You may have also noticed that some of the column names differ only by punctuation, which caused the dataframes to be combined incorrectly:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P93QtXi9uP24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "3ad8d6a9-e534-4989-a710-f2a7a01bd1f8"
      },
      "source": [
        "Trust (Government Corruption)\n",
        "Trust..Government.Corruption."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-20-8c2200e6b46c>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Trust (Government Corruption)\u001b[0m\n\u001b[0m                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOiNwyRyuT99"
      },
      "source": [
        "In the next exercise, we'll update the column names to make them uniform and combine the dataframes again. To clean the column names, we recommend using a technique we haven't covered yet, described in [this Stack Overflow answer](https://stackoverflow.com/questions/39741429/pandas-replace-a-character-in-all-column-names).\n",
        "\n",
        "As you start to work on more data cleaning tasks, you'll inevitably encounter scenarios you don't know specifically how to handle. Stack Overflow is a great place to reference to get answers for these questions, as other people have likely already asked the same question and solicited answers.\n",
        "\n",
        "As a reminder, below is a list of common string methods you can use to clean the columns:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvniRDN7W9ak"
      },
      "source": [
        "|Method|\tDescription|\n",
        "|---|---|\n",
        "|Series.str.split()\t|Splits each element in the Series.\n",
        "|Series.str.strip()\t|Strips whitespace from each string in the Series.\n",
        "|Series.str.lower()\t|Converts strings in the Series to lowercase.\n",
        "|Series.str.upper()\t|Converts strings in the Series to uppercase.\n",
        "|Series.str.get()\t|Retrieves the ith element of each element in the Series.\n",
        "|Series.str.replace()\t|Replaces a regex or string in the Series with another string.\n",
        "|Series.str.cat()\t|Concatenates strings in a Series.\n",
        "|Series.str.extract()\t|Extracts substrings from the Series matching a regex pattern.\n",
        "\n",
        "Let's clean the column names next."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IfNhhACXSfp"
      },
      "source": [
        "**Instructions:**\n",
        "\n",
        "We've already updated the column names for `happiness2017`.\n",
        "\n",
        "- Update the columns names for `happiness2015` and `happiness2016` to match the formatting of the column names in `happiness2017`. Use the following criteria to rename the columns:\n",
        " - All letters should be uppercase.\n",
        " - There should be only one space between words.\n",
        " - There should be no parentheses in column names\n",
        " - For example, the `Health (Life Expectancy)` columns should both be renamed to `HEALTH LIFE EXPECTANCY`.\n",
        "- Use the DataFrame.isnull() and DataFrame.sum() methods to check for missing values. Assign the result to a variable named missing.\n",
        "-Use the `pd.concat()` function to combine `happiness2015`, `happiness2016`, and `happiness2017`. Set the `ignore_index` argument equal to `True` to reset the index in the resulting dataframe. Assign the result to `combined`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RWvYBV9X3uh"
      },
      "source": [
        "# Provided code\n",
        "happiness2017.columns = happiness2017.columns.str.replace('.', ' ').str.replace('\\s+', ' ').str.strip().str.upper()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnVcBAg242OT"
      },
      "source": [
        "# Solution\n",
        "\n",
        "happiness2015.columns = happiness2015.columns.str.replace('(', '').str.replace(')', '').str.strip().str.upper()\n",
        "happiness2016.columns = happiness2016.columns.str.replace('(', '').str.replace(')', '').str.strip().str.upper()\n",
        "combined = pd.concat([happiness2015, happiness2016, happiness2017], ignore_index=True)\n",
        "missing = combined.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mk8ZE6lynyq"
      },
      "source": [
        "## 4. Visualizing Missing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jU3qYaCkyn6E"
      },
      "source": [
        "In the last exercise, we corrected some of the missing values by fixing the column names. Note that we could have cleaned the column names without changing the capitalization. It's good practice, however, to make the capitalization uniform, because a stray uppercase or lowercase letter could've reintroduced missing values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oARepyw6vTtt"
      },
      "source": [
        "We also confirmed there are still values missing:\n",
        "\n",
        "```\n",
        "COUNTRY                          0\n",
        "DYSTOPIA RESIDUAL               22\n",
        "ECONOMY GDP PER CAPITA          22\n",
        "FAMILY                          22\n",
        "FREEDOM                         22\n",
        "GENEROSITY                      22\n",
        "HAPPINESS RANK                  22\n",
        "HAPPINESS SCORE                 22\n",
        "HEALTH LIFE EXPECTANCY          22\n",
        "LOWER CONFIDENCE INTERVAL      335\n",
        "REGION                         177\n",
        "STANDARD ERROR                 334\n",
        "TRUST GOVERNMENT CORRUPTION     22\n",
        "UPPER CONFIDENCE INTERVAL      335\n",
        "WHISKER HIGH                   337\n",
        "WHISKER LOW                    337\n",
        "YEAR                             0\n",
        "dtype: int64\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0X_K8vuFvw4R"
      },
      "source": [
        "We can learn more about where these missing values are located by visualizing them with a [heatmap](https://seaborn.pydata.org/generated/seaborn.heatmap.html), a graphical representation of our data in which values are represented as colors. We'll use the seaborn library to create the heatmap.\n",
        "\n",
        "Note below that we first reset the index to be the `YEAR` column so that we'll be able to see the corresponding year on the left side of the heatmap:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnU6-T-Hv6YK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "0bd901c1-5d24-45b3-8686-8d378d8d8748"
      },
      "source": [
        "import seaborn as sns\n",
        "combined_updated = combined.set_index('YEAR')\n",
        "sns.heatmap(combined_updated.isnull(), cbar=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-8f1e703e6fdb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcombined_updated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'YEAR'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_updated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mset_index\u001b[0;34m(self, keys, drop, append, inplace, verify_integrity)\u001b[0m\n\u001b[1;32m   4553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4555\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of {missing} are in the columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4557\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"None of ['YEAR'] are in the columns\""
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXZluEr9hPcm"
      },
      "source": [
        "To understand this visualization, imagine we took `combined`, highlighted missing values in light gray and all other values in black, and then shrunk it so that was could easily view the entire dataframe at once.\n",
        "\n",
        "Since we concatenated `happiness2015`, `happiness2016`, and `happiness2017` by stacking them, note that the top third of the dataframe corresponds to the 2015 data, the second third corresponds to the 2016 data, and the bottom third corresponds to the 2017 data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keS_rv0YhW2k"
      },
      "source": [
        "We can make the following observations:\n",
        "\n",
        "- No values are missing in the `COUNTRY` column.\n",
        "- There are some rows in the 2015, 2016, and 2017 data with missing values in all columns EXCEPT the `COUNTRY` column.\n",
        "- Some columns only have data populated for one year.\n",
        "- It looks like the `REGION` data is missing for the year 2017.\n",
        "\n",
        "Let's check that the last statement is correct in the next exercise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OL1JISlKhkpU"
      },
      "source": [
        "**Instructions:**\n",
        "\n",
        "- Confirm that the `REGION` column is missing from the 2017 data. Recall that there are 164 rows for the year 2017.\n",
        " - Select just the rows in `combined` in which the `YEAR` column equals 2017. Then, select just the `REGION` column. Assign the result to `regions_2017`.\n",
        " - Use the `Series.isnull()` and `Series.sum()` to calculate the total number of missing values in `regions_2017`, the `REGION` column for 2017. Assign the result to `missing`.\n",
        "- Use the variable inspector to view the results of `missing`. Are all 164 region values missing for the year 2017?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKSiJ77nhkV1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640
        },
        "outputId": "4f8743a3-0559-4512-9e13-bfee8037eb6e"
      },
      "source": [
        "# Answer\n",
        "regions_2017 = combined[combined['YEAR']==2017]['REGION']\n",
        "missing = regions_2017.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2898\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'YEAR'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-ae9dde30d6c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Answer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mregions_2017\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombined\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcombined\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'YEAR'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m2017\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'REGION'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmissing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregions_2017\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2904\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2905\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2906\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2907\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2898\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2900\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2902\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'YEAR'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qij5F-6myoCu"
      },
      "source": [
        "## 5. Using Data From Additional Sources to Fill in Missing Values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXXuX7BxyoNd"
      },
      "source": [
        "In the last exercise, we confirmed that the `REGION` column is missing from the 2017 data. Since we need the regions to analyze our data, let's turn our attention there next.\n",
        "\n",
        "Before we drop or replace any values, let's first see if there's a way we can use other available data to correct the values.\n",
        "\n",
        "1. Check for errors in data cleaning/transformation.\n",
        "2. Use data from additional sources to fill missing values.\n",
        "3. Drop row/column.\n",
        "4. Fill missing values with reasonable estimates computed from the available data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imwV5T08s8VD"
      },
      "source": [
        "Recall once more that each year contains the same countries. Since the regions are fixed values - the region a country was assigned to in 2015 or 2016 won't change - we should be able to assign the 2015 or 2016 region to the 2017 row.\n",
        "\n",
        "In order to do so, we'll use the following strategy:\n",
        "\n",
        "1. Create a dataframe containing all of the countries and corresponding regions from the `happiness2015`, `happiness2016`, and `happiness2017` dataframes.\n",
        "2. Use the `pd.merge()` [function](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html) to assign the `REGION` in the dataframe above to the corresponding country in `combined`.\n",
        "3. The result will have two region columns - the original column with missing values will be named `REGION_x`. The updated column without missing values will be named `REGION_y`. We'll drop `REGION_x` to eliminate confusion.\n",
        "\n",
        "Note that there are other ways to complete this task. We encourage you to explore them on your own.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9u8hZdBgtSUl"
      },
      "source": [
        "**Instructions:**\n",
        "\n",
        "We've already created a dataframe named `regions` containing all of the countries and corresponding regions from the `happiness2015`, `happiness2016`, and `happiness2017` dataframes.\n",
        "\n",
        "- Use the `pd.merge()` function to assign the `REGION` in the `regions` dataframe to the corresponding country in `combined`.\n",
        " - Set the `left` parameter equal to `combined`.\n",
        " - Set the `right` parameter equal to `regions`.\n",
        " - Set the `on` parameter equal to `'COUNTRY'`.\n",
        " - Set the `how` parameter equal to `'left'` to make sure we don't drop any rows from `combined`.\n",
        " - Assign the result back to `combined`.\n",
        "- Use the `DataFrame.drop()` method to drop the original region column with missing values, now named `REGION_x`.\n",
        " - Pass `'REGION_x'` into the `df.drop()` method.\n",
        " - Set the `axis` parameter equal to `1`.\n",
        " - Assign the result back to `combined`.\n",
        "- Use the `DataFrame.isnull()` and `DataFrame.sum()` methods to check for missing values. Assign the result to a variable named `missing`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmTRc_vEtSqx"
      },
      "source": [
        "# Solution\n",
        "\n",
        "combined = pd.merge(left=combined, right=regions, on='COUNTRY', how='left')\n",
        "combined = combined.drop('REGION_x', axis = 1)\n",
        "missing = combined.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2nRcgHHyocw"
      },
      "source": [
        "## 6. Identifying Duplicates Values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z99ez5g9yotY"
      },
      "source": [
        "In the previous screen, we used the 2015 and 2016 data to fill in the missing region values for the 2017 data. Note that we renamed the corrected region column to `REGION` separately to avoid confusion in the following exercises.\n",
        "\n",
        "Before we decide how to handle the rest of our missing values, let's first check our dataframe for duplicate rows.\n",
        "\n",
        "We'll use the `DataFrame.duplicated()` [method](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.duplicated.html) to check for duplicate values. If no parameters are specified, the method will check for any rows in which **all** columns have the same values.\n",
        "\n",
        "Since we should only have one country for each year, we can be a little more thorough by defining rows with ONLY the same country and year as duplicates. To accomplish this, let's pass a list of the `COUNTRY` and `YEAR` column names into the `df.duplicated()` method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MzkboE4FZvn"
      },
      "source": [
        "dups = combined.duplicated(['COUNTRY', 'YEAR'])\n",
        "combined[dups]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0GYtSPuFQC5"
      },
      "source": [
        "Since the dataframe is empty, we can tell that there are no rows with exactly the same country AND year.\n",
        "\n",
        "However, one thing to keep in mind is that the `df.duplicated()` method will only look for exact matches, so if the capitalization for country names isn't exactly the same, they won't be identified as duplicates. To be extra thorough, we can first standardize the capitalization for the `COUNTRY` column and then check for duplicates again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DMURayjF3eg"
      },
      "source": [
        "**Instructions:**\n",
        "\n",
        "- Standardize the capitalization so that all the values in the `COUNTRY` column in `combined` are uppercase.\n",
        " - As an example, ``'India'` should be changed to `'INDIA'`.\n",
        "- Use the `df.duplicated()` method to identify any rows that have the same value in the `COUNTRY` and `YEAR` columns. Assign your result to `dups`.\n",
        "- Use `dups` to index `combined`. Print the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1rsjZt8Fxy3"
      },
      "source": [
        "# Solution\n",
        "\n",
        "combined['COUNTRY'] = combined['COUNTRY'].str.upper()\n",
        "dups = combined.duplicated(['COUNTRY', 'YEAR'])\n",
        "print(combined[dups])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1HaOxAWyo6D"
      },
      "source": [
        "## 7. Correcting Duplicates Values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYv7PFrWypGq"
      },
      "source": [
        "In the previous screen, we standardized the capitalization of the values in the `COUNTRY` column and identified that we actually do have three duplicate rows!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CwHRoY5GV1S"
      },
      "source": [
        "combined['COUNTRY'] = combined['COUNTRY'].str.upper()\n",
        "dups = combined.duplicated(['COUNTRY', 'YEAR'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SY9YmuVGXev"
      },
      "source": [
        "Let's inspect all the rows for `SOMALILAND REGION` in `combined`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJrlRIzyGcIM"
      },
      "source": [
        "combined[combined['COUNTRY'] == 'SOMALILAND REGION']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LT1RZss1Gf8-"
      },
      "source": [
        "Now, we can see that there are two rows for 2015, 2016, and 2017 each."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nT49q4BTGnoM"
      },
      "source": [
        "Next, let's use the `df.drop_duplicates()` [method](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop_duplicates.html) to drop the duplicate rows. Like the `df.duplicated()` method, the `df.drop_duplicates()` method will define duplicates as rows in which **all** columns have the same values. We'll have to specify that rows with the same values in only the `COUNTRY` and `YEAR` columns should be dropped.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySaGNf_NG4iw"
      },
      "source": [
        "It's also important to note that by default, the `drop_duplicates()` method will only keep the first duplicate row. To keep the last duplicate row, set the `keep` parameter to `'last'`. Sometimes, this will mean sorting the dataframe before dropping the duplicate rows.\n",
        "\n",
        "In our case, since the second duplicate row above contains more missing values than the first row, we'll keep the first row."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7whKOt4HGMf"
      },
      "source": [
        "**Instructions:**\n",
        "\n",
        "- Use the `df.drop_duplicates()` method to drop rows with more than one country for each year. Assign the result back to `combined`.\n",
        " - Pass a list containing the `COUNTRY` and `YEAR` columns into the `drop_duplicates()` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCFmhTkfHN-P"
      },
      "source": [
        "# Provided code\n",
        "combined['COUNTRY'] = combined['COUNTRY'].str.upper()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tK_qhQVA5Ddk"
      },
      "source": [
        "# Solution\n",
        "\n",
        "combined = combined.drop_duplicates(['COUNTRY', 'YEAR'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2Y-anKrypRD"
      },
      "source": [
        "## 8. Handle Missing Values by Dropping Columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ILnoLu_41PE"
      },
      "source": [
        "Now that we've corrected the duplicate values in the dataframe, let's turn our attention back to the rest of our missing values. So far, to correct missing values we:\n",
        "\n",
        "1. Corrected the errors we made when combining our dataframes.\n",
        "2. Used the 2015 and 2016 region values to fill in the missing regions for 2017.\n",
        "\n",
        "Many of the methods in pandas are designed to exclude missing values without removing them, so at this point, we could leave the rest of the missing values as is, depending on the question we're trying to answer.\n",
        "\n",
        "However, leaving missing values in the dataframe could cause issues with other transformation tasks and change the distribution of our data set. Also note that missing data has to be dropped or replaced to work with machine learning algorithms, so if you're interested in continuing in the data science path, it's important to know how to handle them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkSDGi8941hF"
      },
      "source": [
        "Next, we'll consider dropping columns with missing data:\n",
        "\n",
        "1. Check for errors in data cleaning/transformation.\n",
        "2. Use data from additional sources to fill missing values.\n",
        "3. Drop row/column.\n",
        "4. Fill missing values with reasonable estimates computed from the available data.\n",
        "\n",
        "First, let's confirm how many missing values are now left in the dataframe:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERQO_ONB5L6W"
      },
      "source": [
        "combined.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5lFiOQJ5Mem"
      },
      "source": [
        "We can see above that a couple columns contain over 300 missing values. Let's start by analyzing these columns since they account for most of the missing values left in the dataframe.\n",
        "\n",
        "When deciding if you should drop a row or column, carefully consider whether you'll lose information that could alter your analysis. Instead of just saying, \"If x percentage of the data is missing, we'll drop it.\", it's better to also ask the following questions:\n",
        "\n",
        "1. Is the missing data needed to accomplish our end goal?\n",
        "2. How will removing or replacing the missing values affect our analysis?\n",
        "\n",
        "To answer the first question, let's establish our end goal:\n",
        "\n",
        "- End Goal: We want to analyze happiness scores and the factors that contribute to happiness scores by year and region.\n",
        "\n",
        "Since missing values make up more than half of the following columns and we don't need them to accomplish our end goal, we'll drop them:\n",
        "\n",
        "- `STANDARD ERROR`\n",
        "- `LOWER CONFIDENCE INTERVAL`\n",
        "- `UPPER CONFIDENCE INTERVAL`\n",
        "- `WHISKER HIGH`\n",
        "- `WHISKER LOW`\n",
        "\n",
        "We'll use the `DataFrame.drop()` [method](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html) to drop them next.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFM4G2204QhH"
      },
      "source": [
        "**Instructions:**\n",
        "\n",
        "- Use the `df.drop()` method to drop the columns in `columns_to_drop`.\n",
        " - Pass `columns_to_drop` into the `df.drop()` method.\n",
        " - Set the `axis` parameter equal to `1`.\n",
        " - Assign the result back to `combined`.\n",
        "- Use the `df.isnull()` and `df.sum()` methods to calculate the number of missing values for each column. Assign the result to `missing`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5RRf4sC9tO-"
      },
      "source": [
        "# Provided code\n",
        "\n",
        "columns_to_drop = ['LOWER CONFIDENCE INTERVAL', 'STANDARD ERROR', 'UPPER CONFIDENCE INTERVAL', 'WHISKER HIGH', 'WHISKER LOW']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iTlUUG4yptr"
      },
      "source": [
        "## 9. Handle Missing Values by Dropping Columns Continued"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bY9j5Zq3ypdu"
      },
      "source": [
        "In the last exercise, we used the `df.drop()` method to drop columns we don't need for our analysis.\n",
        "\n",
        "However, as you start working with bigger datasets, it can sometimes be tedious to create a long list of column names to drop. Instead we can use the `DataFrame.dropna()` [method](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html) to complete the same task.\n",
        "\n",
        "By default, the `dropna()` method will drop rows with any missing values. To drop columns, we can set the `axis` parameter equal to `1`, just like with the `df.drop()` method:\n",
        "\n",
        "```\n",
        "df.dropna(axis=1)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XlT1UHrMukv"
      },
      "source": [
        "However, this would result in dropping columns with any missing values - we only want to drop certain columns. Instead, we can also use the `thresh` parameter to only drop columns if they contain below a certain number of non-null values.\n",
        "\n",
        "So far, we've used the `df.isnull()` method to confirm the number of missing values in each column. To confirm the number of values that are NOT missing, we can use the `DataFrame.notnull()` [method](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.notnull.html):\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyGEtE_AM63u"
      },
      "source": [
        "combined.notnull().sum().sort_values()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElmokmPpM9zm"
      },
      "source": [
        "Above, we can see that the columns we'd like to drop - `LOWER CONFIDENCE INTERVAL`, `STANDARD ERROR`, `UPPER CONFIDENCE INTERVAL`, `WHISKER HIGH`, and `WHISKER LOW` - only contain between 155 and 158 non null values. As a result, we'll set the `thresh` parameter equal to 159 in the `df.dropna()` method to drop them.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYRKB9JDNFwn"
      },
      "source": [
        "**Instructions:**\n",
        "\n",
        "- Use the `df.dropna()` method to drop all columns in `combined` with 159 or less non null values.\n",
        " - Set the `thresh` argument equal to `159` and the `axis` parameter equal to `1`.\n",
        "- Use the `df.isnull()` and `df.sum()` methods to calculate the number of missing values for each column. Assign the result to `missing`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cU5uvUE-NYuN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CIxL9jk3-P6"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2avhv1FSzbgG"
      },
      "source": [
        "## 10. Analyzing Missing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6FKXvq4zcEh"
      },
      "source": [
        "In the last exercise, we dropped columns we don't need for our analysis and confirmed that a couple columns still have missing values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7V8FbKoFfLB"
      },
      "source": [
        "combined.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV4sGosQFjKC"
      },
      "source": [
        "To make a decision about how to handle the rest of the missing data, we'll analyze if it's better to just drop the rows or replace the missing values with other values.\n",
        "\n",
        "Let's return to the following questions:\n",
        "\n",
        "1. Is the missing data needed to accomplish our end goal?\n",
        " - Yes, we need the data to accomplish our goal of analyzing happiness scores and contributing factors by region and year.\n",
        "2. How will removing or replacing the missing values affect our analysis?\n",
        "\n",
        "Let's break the second question down into a couple more specific questions:\n",
        "\n",
        "1. What percentage of the data is missing?\n",
        "2. Will dropping missing values cause us to lose valuable information in other columns?\n",
        "3. Can we identify any patterns in the missing data?\n",
        "\n",
        "*Question: What percentage of the data is missing?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C03TLcizGBom"
      },
      "source": [
        "As we saw when looking at the results of `combined.isnull().sum()` above, if missing values exist in a column of our dataframe, they account for about 4 percent of the total values (19 missing out of 489 values per column).\n",
        "\n",
        "Generally speaking, the lower the percentage of missing values, the less likely dropping them will significantly impact the analysis.\n",
        "\n",
        "*Question: Will dropping missing values cause us to lose valuable information in other columns?*\n",
        "\n",
        "To answer this question, let's visualize the missing data once more. Note below that before we create the heatmap, we first set the index of `combined` to the `REGION` column and sort the values:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGi-XR85waus"
      },
      "source": [
        "sorted = combined.set_index('REGION').sort_values(['REGION', 'HAPPINESS SCORE'])\n",
        "sns.heatmap(sorted.isnull(), cbar=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izFjZO3qHes8"
      },
      "source": [
        "As a reminder, in the heatmap above, the missing values are represented with light gray and all other values with black. From this visualization, we can confirm that if the data is missing, it's missing in almost every column. We'll conclude that dropping the missing values won't cause us to lose valuable information in other columns.\n",
        "\n",
        "*Question: Can we identify any patterns in the missing data?*\n",
        "\n",
        "From the visualization above, we can also identify that only three regions contain missing values:\n",
        "\n",
        "- Sub-Saharan Africa\n",
        "- Middle East and Northern Africa\n",
        "- Latin America and Carribbean\n",
        "\n",
        "The Sub-Saharan Africa region contains the most missing values, accounting for about 9 percent of that regions's values. Since we'd like to analyze the data according to region, we should also think about how these values impact the analysis for this region specifically.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeGQ5-KmzcOQ"
      },
      "source": [
        "## 11. Handling Missing Values with Imputation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5t-0vZQLzcVZ"
      },
      "source": [
        "In the last screen, we confirmed:\n",
        "\n",
        "- Only about 4 percent of the values in each column are missing.\n",
        "- Dropping rows with missing values won't cause us to lose information in other columns.\n",
        "\n",
        "As a result, it may be best to drop the remaining missing values.\n",
        "\n",
        "However, before we make a decision, let's consider handling the missing values by replacing them with estimated values, also called imputation.\n",
        "\n",
        "1. Check for errors in data cleaning/transformation.\n",
        "2. Use data from additional sources to fill missing values.\n",
        "3. Drop row/column.\n",
        "4. Fill missing values with reasonable estimates computed from the available data.\n",
        "\n",
        "There are many options for choosing the replacement value, including:\n",
        "\n",
        "- A constant value\n",
        "- The mean of the column\n",
        "- The median of the column\n",
        "- The mode of the column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kc8f8NMzw4tR"
      },
      "source": [
        "For non-numeric columns, common replacement values include the most frequent value or a string like \"Unknown\" that is used to treat missing values as a separate category.\n",
        "\n",
        "For numeric columns, it's very common to replace missing values with the mean. Since the rest of the columns in `combined` with missing data are all numeric, we'll explore this option next.\n",
        "\n",
        "First, let's build some intuition around this technique by analyzing how replacing missing values with the mean affects the distribution of the data. In order to do so, we'll use the `Series.fillna()` [method](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.fillna.html) to replace the missing values with the mean."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6S5cSiXxVd3"
      },
      "source": [
        "Note that we must pass the replacement value into the `Series.fillna()` method. For example, if we wanted to replace all of the missing values in the `HAPPINESS SCORE` column with `0`, we'd use the following syntax:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UoUKZr01Lpc"
      },
      "source": [
        "combined[`HAPPINESS SCORE`].fillna(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRa4Mh361O9T"
      },
      "source": [
        "Next, let's replace the missing happiness scores with the mean."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYBsgFa21QQP"
      },
      "source": [
        "**Instructions:**\n",
        "\n",
        "- Use the `Series.mean()` [method]() to calculate the mean of the `HAPPINESS SCORE` column. Assign the result to `happiness_mean`. Print `happiness_mean`.\n",
        "- Use the `Series.fillna()` method to replace all the missing values in the `HAPPINESS SCORE` column with `happiness_mean`. Assign the result to a new column named `HAPPINESS SCORE UPDATED`.\n",
        "- Print the mean of `HAPPINESS SCORE UPDATED`.\n",
        "- Based on the results of this exercise, try to answer the question below:\n",
        " - Did replacing missing values with the mean of a series cause the mean to change?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDE5LMAv2p-S"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7CDAGoXzcdH"
      },
      "source": [
        "## 12. Dropping Rows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7AVxNgnzlMy"
      },
      "source": [
        "In the last exercise, we confirmed that replacing missing values with the Series mean doesn't change the mean of the Series.\n",
        "\n",
        "If we were to plot the distributions before and after replacing the missing values with the mean, we'd see that the shape of the distribution changes as more values cluster around the mean. Note that the mean is represented with the red and green lines in the plots below:\n",
        "\n",
        "![img](https://s3.amazonaws.com/dq-content/347/Happiness_means_original.png)\n",
        "![img](https://s3.amazonaws.com/dq-content/347/Happiness_means.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaEtujIA25L-"
      },
      "source": [
        "As we decide to use this approach, we should ask the following questions - are the missing happiness scores likely to be close to the mean? Or is it more *likely* that the scores are very high or very low? If the missing values lie at extremes, the mean won't be a good estimate for them.\n",
        "\n",
        "![img](https://s3.amazonaws.com/dq-content/347/Missing_values.svg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwTAuqEEKAVQ"
      },
      "source": [
        "Recall that when we visualized the missing data, we determined that the Sub-Saharan Africa region contained the most missing values. Since we'd like to analyze the data according to region, let's look more closely at the means for each region:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxiQmR8sKBhq"
      },
      "source": [
        "combined.pivot_table(index='REGION', values='HAPPINESS SCORE', margins=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysJAD4TjKEq5"
      },
      "source": [
        "As a reminder, the All row in the table above represents the mean happiness score for the whole world - the value that we used to replace our missing values. We can see that the world mean happiness score, 5.370728, is over 1 point higher than the mean happiness score for the Sub-Saharan Africa region, 4.150957.\n",
        "\n",
        "Also, if we think about the reasons why a country may not have participated in the happiness survey - war, natural disaster, etc - many of them would likely result in a lower happiness score than even the region's mean. We'll conclude that the mean for the whole world wouldn't be a good estimate for them.\n",
        "\n",
        "As a result, we'll decide that of these two options, it's better to drop the rows with missing values. Let's do that next."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GftX4QtLKQDl"
      },
      "source": [
        "**Instructions:**\n",
        "\n",
        "- Use the `DataFrame.dropna()` method to drop any remaining rows with missing values. Assign the result back to `combined`.\n",
        "- Use the `df.isnull()` and `df.sum()` methods to confirm there are no missing values left in combined. Assign the result to `missing`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lJSmTlKKXsi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRNQROk3SFqZ"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "In the last step, we concluded that in this case, it was better to drop the remaining rows with missing values rather than replace the missing values with the mean.\n",
        "\n",
        "However, it's also good to know that other techniques for handling missing values do exist. Since this mission is meant to be an introduction to this topic, we didn't cover them, but if you're interested in learning more, you can start [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html).\n",
        "\n",
        "\n",
        "Although there is no perfect way to handle missing values and each situation is different, now we know the basic techniques and built some intuition around them to better inform our decisions. Below is the workflow we used to clean missing values:\n",
        "\n",
        "1. Check for errors in data cleaning/transformation.\n",
        "2. Use data from additional sources to fill missing values.\n",
        "3. Drop row/column.\n",
        "4. Fill missing values with reasonable estimates computed from the available data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjkPcTemSccE"
      },
      "source": [
        "We also started to set a more defined data cleaning workflow, in which we:\n",
        "\n",
        "- Set a goal for the project.\n",
        "- Researched and tried to understand the data.\n",
        "- Determined what data was needed to complete our analysis.\n",
        "- Added columns.\n",
        "- Cleaned specific data types.\n",
        "- Combined data sets.\n",
        "- Removed duplicate values.\n",
        "- Handled the missing values.\n",
        "\n",
        "Next, we'll synthesize and practice what we've learned in the Guided Project.\n",
        "\n"
      ]
    }
  ]
}